{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "viridis = matplotlib.cm.get_cmap('viridis')\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "from evaluate_n_labeled import n_labeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########\n",
      "Reconst loss : mse\n",
      "ARS  0.15307305753958494 0.15307305753958494\n",
      "Ind of experiment:  701 | run:  194\n",
      "N runs:  949\n",
      "-----------\n",
      "[list([[32, 32, 32, 32], [5, 5, 3, 3], [2, 2, 2, 2], [0, 0, 0, 0], 4])\n",
      " list([0.0001111111111111111, 1e-05, 0.7402157310303168, 0.99, 2, 130.6469387755102, 1])\n",
      " {'include_KM:': False, 'include_KL': False, 'simulated_mode': False, 'use_vgg': False, 'use_dd': True, 'batchnorm': False, 'include_KM': True, 'restore_mode': False, 'include_MMD': False}\n",
      " {'pretrain_epochs': 100, 'self_supervise': False, 'delta': 0.01, 'n_clusters': 2, 'update_interval': 1, 'alpha': 1, 'pretrain_simulated': 0}\n",
      " 'mse' 'lrelu']\n",
      "##########\n",
      "Reconst loss : mse\n",
      "ARS  0.18458486236764807 0.18458486236764807\n",
      "Ind of experiment:  143 | run:  195\n",
      "N runs:  883\n",
      "-----------\n",
      "[list([[16, 16, 16, 16], [5, 5, 3, 3], [2, 2, 2, 2], [0, 0, 0, 0], 4])\n",
      " list([0.0004444444444444444, 0.01, 0.7179302107890106, 0.99, 7, 138.80612244897958, 2])\n",
      " {'include_KM:': False, 'include_KL': False, 'simulated_mode': False, 'use_vgg': False, 'use_dd': True, 'batchnorm': True, 'include_KM': True, 'restore_mode': False, 'include_MMD': False}\n",
      " {'pretrain_epochs': 300, 'self_supervise': False, 'delta': 0.01, 'n_clusters': 2, 'update_interval': 150, 'alpha': 1, 'pretrain_simulated': 0}\n",
      " 'mse' 'relu']\n"
     ]
    }
   ],
   "source": [
    "algo = \"convae\"\n",
    "arch = \"static\"\n",
    "size = \"128\"\n",
    "datasets = [\"simulated\", \"simulated\"]\n",
    "runs = [\"194\", \"195\"]\n",
    "\n",
    "losses_all = []\n",
    "performance_all = []\n",
    "params_all = []\n",
    "to_print = [13, 84, 116]\n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    data = datasets[i]\n",
    "    run = runs[i]\n",
    "    base_str = \"../results/randomsearch_\"+algo+\"_\"+data+\"_\"+size+\"_clster/\"\n",
    "\n",
    "    param_vals = np.load(base_str+\"run_\"+run+\"/hyperparam_vals_\"+arch+\".npy\")\n",
    "    #losses = np.load(base_str+\"run_\"+run+\"/loss_vals_\"+arch+\".npy\")\n",
    "    performance = np.load(base_str+\"run_\"+run+\"/performance_\"+arch+\".npy\")\n",
    "    #print(performance[:,0].max())\n",
    "    #print(performance)\n",
    "    n_highest = 20\n",
    "    \n",
    "    \"\"\"\n",
    "    to_del = []\n",
    "    for j, p in enumerate(performance):\n",
    "        if len(p[0]) == 2:\n",
    "            print(j)\n",
    "            to_del.append(j)\n",
    "\n",
    "    performance = np.array(np.delete(performance, to_del, axis=0))\n",
    "    param_vals = np.array(np.delete(param_vals, to_del, axis=0))\n",
    "    losses = np.array(np.delete(losses, to_del, axis=0))\n",
    "    \"\"\"\n",
    "    \n",
    "    rand_score = np.array([p for p in performance[:, 0]])\n",
    "    rand_sort_ind = np.argsort(-rand_score) \n",
    "    sorted_rand_score = rand_score[rand_sort_ind]\n",
    "    #print(param_vals[to_print[i]])\n",
    "    #print(performance[to_print[i]])\n",
    "    \n",
    "    #lx  = losses[:, 0]\n",
    "    #lx = np.where(lx > 0, lx, 1e5)\n",
    "    #best_lx = lx.min(1)\n",
    "    #ind_sort_lx = np.argsort(best_lx)\n",
    "    #print(ind_sort_lx[0])\n",
    "    #print(best_lx[ind_sort_lx[0]])\n",
    "    #print(param_vals[ind_sort_lx[0]])\n",
    "    print(\"##########\")\n",
    "    #print(losses.shape)\n",
    "    p = 0\n",
    "    #best_lx = losses[rand_sort_ind[p]][0]\n",
    "    #print(performance[rand_sort_ind][:,0])\n",
    "    #print(\"Max reconst loss: \", best_lx.max(), best_lx.shape)\n",
    "    print(\"Reconst loss :\", param_vals[rand_sort_ind[p]][-2])\n",
    "    print(\"ARS \", rand_score[rand_sort_ind[p]], performance[rand_sort_ind[p]][0])\n",
    "    print(\"Ind of experiment: \", rand_sort_ind[p], \"| run: \", run)\n",
    "    print(\"N runs: \", len(rand_sort_ind))\n",
    "    print(\"-----------\")\n",
    "    print(param_vals[rand_sort_ind[0]])\n",
    "\n",
    "    #losses = losses[rand_sort_ind][:n_highest]\n",
    "    #losses_all.append(losses)\n",
    "    \n",
    "    performance = performance[rand_sort_ind][:n_highest]\n",
    "    \n",
    "    performance_all.append(performance)\n",
    "    \n",
    "    param_vals = param_vals[rand_sort_ind][:n_highest]\n",
    "    params_all.append(param_vals)\n",
    "    \n",
    "    #print(len(sorted_proton_test_f1))\n",
    "    #print(sorted_proton_test_f1)\n",
    "    #print(param_vals[p])\n",
    "    #print(\"performance : \", performance[p])\n",
    "    #print(\"test f1\", performance[p][1][0].mean())\n",
    "    #print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(losses_all[0][15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "realevent\n",
      "test f1:  0.8145900844096133\n",
      "[list([[8, 8, 32, 32], [5, 5, 3, 3], [2, 2, 2, 2], [0, 0, 0, 0], 4])\n",
      " list([0.001, 0.0001, 0.7488448785798214, 0.99, 200, 1.188888888888889])\n",
      " {'batchnorm': False, 'include_KL': False, 'include_MMD': True, 'restore_mode': False, 'include_KM:': False, 'simulated_mode': False, 'use_vgg': False, 'use_dd': True}\n",
      " {} 'mse' 'relu']\n",
      "realevent\n",
      "test f1:  0.80448033122487\n",
      "[list([[8, 8, 16, 16], [5, 5, 3, 3], [2, 2, 2, 2], [0, 0, 0, 0], 4])\n",
      " list([0.01, 0.001, 0.7989680735022695, 0.99, 100, 3.366666666666667])\n",
      " {'batchnorm': True, 'include_KL': False, 'include_MMD': False, 'restore_mode': False, 'include_KM:': False, 'simulated_mode': False, 'use_vgg': False, 'use_dd': True}\n",
      " {} 'mse' 'lrelu']\n"
     ]
    }
   ],
   "source": [
    "for i, params in enumerate(params_all):\n",
    "    which = 0\n",
    "    print(datasets[i])\n",
    "    print(\"test f1: \", performance_all[i][which][1][0].mean())\n",
    "    print(params[which])\n",
    "    \n",
    "    \"\"\"\n",
    "    for j, p in enumerate(params):\n",
    "        if 3 == p[0][1][0]:\n",
    "            f1 = performance_all[i][j][1][0].mean()\n",
    "            arch = params_all[i][j]\n",
    "            print(\"#############\")\n",
    "            print(\"test f1 : \", f1)\n",
    "            print(arch)\n",
    "            print(\"#############\")\n",
    "    print(\"--------------------------\")\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8356062231211628"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_all[0][0][1][0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"Simulated\", \"Filtered\", \"Full\"]\n",
    "loss = [\"Reconstruction\", \"Latent\"]\n",
    "\n",
    "fig, axs = plt.subplots(ncols = len(data), nrows=2, figsize=(15, 20))\n",
    "for i in range(len(loss)):\n",
    "    for j in range(len(data)):\n",
    "        ax = axs[i, j]\n",
    "        ax_t = ax.twiny()\n",
    "        performance = np.array([p[1][0].mean() for p in performance_all[j]])\n",
    "        if i == 0:\n",
    "            losses = np.zeros(losses_all[j].shape[0])\n",
    "            for k, lc in enumerate(losses_all[j]):\n",
    "                loss = lc[0]\n",
    "                non_zero = np.nonzero(loss)[0]\n",
    "                to_log = loss[non_zero[-1]]\n",
    "                #print(to_log)\n",
    "                losses[k] = to_log\n",
    "            what_loss = losses > 1     \n",
    "        else:\n",
    "            losses = np.zeros(losses_all[j].shape[0])\n",
    "            for k, lc in enumerate(losses_all[j]):\n",
    "                loss = lc[1]\n",
    "                non_zero = np.nonzero(loss)\n",
    "                if len(non_zero[0]) == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    to_log = loss[non_zero[0][-1]]\n",
    "                    #print(to_log)\n",
    "                    losses[k] = to_log\n",
    "            what_loss = np.zeros(len(losses))\n",
    "            for k, param in enumerate(params_all[j]):\n",
    "                loss = param[2][\"include_MMD\"]\n",
    "                what_loss[k] = 1 if loss else 0 \n",
    "            \n",
    "            what_loss = what_loss.astype(bool)\n",
    "            \n",
    "        outlier_filter = losses < 1e4\n",
    "        one_plot = [losses[np.logical_and(what_loss, outlier_filter)], performance[np.logical_and(what_loss, outlier_filter)]]\n",
    "        two_plot = [losses[np.logical_and(~what_loss, outlier_filter)], performance[np.logical_and(~what_loss, outlier_filter)]]\n",
    "\n",
    "        #print(losses.shape, performance.shape)\n",
    "        ax.scatter(one_plot[0], one_plot[1])\n",
    "        ax_t.scatter(two_plot[0], two_plot[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_crossent(x, y=1): return -(y*np.log(x) + (1-y)*np.log(1-x))\n",
    "def mse(x, y=1): return np.power(x-y, 2)\n",
    "\n",
    "x = np.linspace(-3, 3, 300)\n",
    "y = [0.7, ] #np.linspace(1e-3, 1, 5)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "for y_ in y:\n",
    "    ax[0].plot(x, binary_crossent(x, y_), linewidth=3, color=viridis(0.4))\n",
    "    ax[0].plot([y_,]*2,[0.5, 0.8], label=\"y= \"+str(y_), color=viridis(0.8))\n",
    "    ax[0].set_xlabel(\"x\", size=20)\n",
    "    ax[0].set_ylabel(\"Binary crossentropy\", size=20)\n",
    "    ax[0].legend(fontsize=15)\n",
    "    \n",
    "    ax[1].plot(x, mse(x, y_), linewidth=3, color=viridis(0.4))\n",
    "    ax[1].plot([y_,]*2,[-0.4, 0.8], label=\"y= \"+str(y_), color=viridis(0.8))\n",
    "    ax[1].set_xlabel(\"x\", size=20)\n",
    "    ax[1].set_ylabel(\"Squared error\", size=20)\n",
    "    ax[1].legend(fontsize=15)\n",
    "    \n",
    "    ax[0].tick_params(axis='both', which='major', labelsize=15)\n",
    "    ax[1].tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(performance.shape)\n",
    "print(\"experiments, train/test, scores, classes\")\n",
    "print(performance[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [print(h[-1], h[0]) for h in param_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_names = [r\"$L_x$\", r\"$L_z/\\beta$\"]\n",
    "n_plot = 10\n",
    "fig, ax = plt.subplots(ncols=losses.shape[1], figsize=(16, 9))\n",
    "colors = viridis(np.linspace(0.3, max(sorted_proton_test_f1), n_plot))\n",
    "ax2 = ax[0].twinx()\n",
    "ax3 = ax[1].twinx()\n",
    "\n",
    "#fig.suptitle(\"Loss curves from simulated parameter search\", size=35)\n",
    "\n",
    "label_size = 30\n",
    "ax[1].set_ylabel(r\"Maximum mean discrepancy: $\\bigtriangledown$\", size=label_size)\n",
    "ax3.set_ylabel(r\"Kullback-Leibler : $\\bullet$\", size=label_size)\n",
    "\n",
    "ax2.set_ylabel(r\"Mean squared error: $\\bigtriangleup$\", size=label_size)\n",
    "ax[0].set_ylabel(r\"Binary crossentropy: $\\diamond$\", size=label_size)\n",
    "\n",
    "ax[0].set_title(r\"Reconstruction loss: \"+loss_names[0], size=30)\n",
    "ax[1].set_title(\"Latent loss: \"+loss_names[1], size=30)\n",
    "\n",
    "for j in range(n_plot):\n",
    "    if j == 5:\n",
    "        continue\n",
    "    if j == 8:\n",
    "        continue\n",
    "    for i, a in enumerate(ax):\n",
    "        if i == 1:\n",
    "            beta = param_vals[j][1][0]\n",
    "            mode = param_vals[j][2]\n",
    "            which = None\n",
    "            for l, v in mode.items():\n",
    "                if v:\n",
    "                    if l != \"batchnorm\":\n",
    "                        which = l\n",
    "            if which == \"include_KL\":\n",
    "                fmt = \"o-\"\n",
    "                a = ax3\n",
    "            elif which == \"include_MMD\":\n",
    "                fmt = \"v-\"\n",
    "                a = ax[1]\n",
    "            else:\n",
    "                fmt = \"-\"\n",
    "        else:\n",
    "            beta = 1\n",
    "            if np.any(losses[j, i, :][50:]>1e1):\n",
    "                fmt = \"^-\"\n",
    "            else:\n",
    "                fmt = \"D-\"\n",
    "                a = ax2\n",
    "        loss = losses[j, i, 2:]/beta\n",
    "        a.semilogy(\n",
    "            np.arange(loss.shape[0]),\n",
    "            loss,\n",
    "            fmt,\n",
    "            markevery=250,\n",
    "            color=colors[j],\n",
    "            markersize=15,\n",
    "            linewidth=0.5,\n",
    "            alpha=0.6\n",
    "        )\n",
    "        print(sorted_proton_test_f1[j])\n",
    "        a.tick_params(axis='both', which='major', labelsize=15)\n",
    "        a.set_xlabel(\"epoch\", size=20)\n",
    "\n",
    "#ax[0].set_yscale(\"linear\")\n",
    "ax[0].set_yticks([1e4, 1e3, 1e2])\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../plots/simulated_clf/randomsearch_loss\"+algo+\".png\")\n",
    "plt.savefig(\"../plots/simulated_clf/randomsearch_loss\"+algo+\".pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = [\n",
    "    \"ARS\",\n",
    "    r\"$L_x$\",\n",
    "    r\"$L_z$\",\n",
    "    \"N parameters\",\n",
    "    \"largest kernel\",\n",
    "    \"N layers\",\n",
    "    \"latent dimension\",\n",
    "    \"latent loss\",\n",
    "    \"reconstruction loss\",\n",
    "    \"activation function\",\n",
    "    \"batchnorm\",\n",
    "    \"DD_dense\",\n",
    "    \"lambda\",\n",
    "    \"pretrain epochs\",\n",
    "    r\"$\\beta$\",\n",
    "    r\"$\\beta_1$\",\n",
    "    r\"$\\eta$\",\n",
    "\n",
    "]\n",
    "\n",
    "categorical_cols = [\n",
    "    \"latent loss\",\n",
    "    \"reconstruction loss\",\n",
    "    \"activation function\",\n",
    "]\n",
    "\n",
    "dataframes = []\n",
    "for j, d in enumerate(datasets):\n",
    "    param_performance = np.zeros((performance.shape[0], len(columns))).astype(object)\n",
    "    performance = performance_all[j]\n",
    "    param_vals = params_all[j]\n",
    "    losses = losses_all[j]\n",
    "    print(performance.shape[0])\n",
    "    for i in range(performance.shape[0]):\n",
    "        config = param_vals[i]\n",
    "        #print(performance.shape)\n",
    "        p_f1 = performance[i][0].mean() #sorted_proton_test_f1[i]\n",
    "        n_params = 0\n",
    "        for f, k in zip(config[0][0], config[0][1]):\n",
    "            n_params += k**2*f\n",
    "        n_layers = config[0][4]\n",
    "        end_size = 80/(2**n_layers)\n",
    "        beta1 = config[1][2]\n",
    "        beta = config[1][1]\n",
    "        latent_dim = config[1][4]\n",
    "        eta=config[1][1]\n",
    "        mode_config = config[2]\n",
    "        batchnorm = mode_config[\"batchnorm\"]\n",
    "        clust_config = config[3]\n",
    "        dd_dense = config[1][-1]\n",
    "        lmbd = config[1][-2]\n",
    "        pretrain_e = clust_config[\"pretrain_epochs\"]\n",
    "        \n",
    "        lx  = losses[i, 0]\n",
    "        lx = np.where(lx > 0, lx, 1e5)\n",
    "        \n",
    "        best_lx = lx.min()\n",
    "        \n",
    "        lz  = losses[i, 1]\n",
    "        lz = np.where(lz > 0, lz, 1e5)\n",
    "        best_lz = lz.min()\n",
    "\n",
    "        latent_loss = \"mmd\" if mode_config[\"include_MMD\"] else \"kld\"\n",
    "        if latent_loss == \"kld\":\n",
    "            latent_loss = latent_loss if mode_config[\"include_KL\"] else \"none\"\n",
    "\n",
    "        reconst_loss = config[4]\n",
    "        if reconst_loss is None:\n",
    "            reconst_loss = \"bce\"\n",
    "\n",
    "        activation = config[5] \n",
    "        param_performance[i] = [\n",
    "            p_f1,\n",
    "            best_lx,\n",
    "            best_lz,\n",
    "            n_params,\n",
    "            config[0][1][0],\n",
    "            n_layers,\n",
    "            latent_dim,\n",
    "            latent_loss,\n",
    "            reconst_loss,\n",
    "            activation,\n",
    "            batchnorm,\n",
    "            dd_dense,\n",
    "            lmbd,\n",
    "            pretrain_e,\n",
    "            beta,\n",
    "            beta1,\n",
    "            eta,\n",
    "        ]\n",
    "\n",
    "    perf_df = pd.DataFrame(param_performance, columns=columns)\n",
    "    dataframes.append(perf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = dataframes[1].copy()\n",
    "for c in categorical_cols:\n",
    "    dummy = pd.get_dummies(test_df[c])\n",
    "    test_df = pd.concat([test_df, dummy], axis=1)\n",
    "    test_df = test_df.drop(c, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARS</th>\n",
       "      <th>$L_x$</th>\n",
       "      <th>$L_z$</th>\n",
       "      <th>N parameters</th>\n",
       "      <th>largest kernel</th>\n",
       "      <th>N layers</th>\n",
       "      <th>latent dimension</th>\n",
       "      <th>batchnorm</th>\n",
       "      <th>DD_dense</th>\n",
       "      <th>lambda</th>\n",
       "      <th>pretrain epochs</th>\n",
       "      <th>$\\beta$</th>\n",
       "      <th>$\\beta_1$</th>\n",
       "      <th>$\\eta$</th>\n",
       "      <th>none</th>\n",
       "      <th>mse</th>\n",
       "      <th>lrelu</th>\n",
       "      <th>relu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.184585</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>1088</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>138.806</td>\n",
       "      <td>300</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.71793</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.140921</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>2176</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>53.1347</td>\n",
       "      <td>300</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.615548</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.104322</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>2176</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.102032</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>2176</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>20.498</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.598485</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0962859</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>1376</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>49.0551</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.390532</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0863167</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>2176</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.488072</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0853191</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>976</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>77.6122</td>\n",
       "      <td>200</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.238501</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0786247</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>2176</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>187.761</td>\n",
       "      <td>200</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.692263</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0785352</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>2176</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>53.1347</td>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.221878</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0748281</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>1376</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>175.522</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.842695</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0743384</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>2176</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>44.9755</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.210227</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0735996</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>1376</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>179.602</td>\n",
       "      <td>300</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0685182</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>2176</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>20.498</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.612688</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0680857</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>1376</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>110.249</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.923865</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0647884</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>2176</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>134.727</td>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.896138</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0618727</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>1088</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>40.8959</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.302891</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0613304</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>2176</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>179.602</td>\n",
       "      <td>300</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.78403</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0602345</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>1088</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>179.602</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.250402</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0580285</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>544</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>81.6918</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.423839</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.057791</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>976</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>167.363</td>\n",
       "      <td>300</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.855116</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ARS   $L_x$   $L_z$ N parameters largest kernel N layers  \\\n",
       "0    0.184585  100000  100000         1088              5        4   \n",
       "1    0.140921  100000  100000         2176              5        4   \n",
       "2    0.104322  100000  100000         2176              5        4   \n",
       "3    0.102032  100000  100000         2176              5        4   \n",
       "4   0.0962859  100000  100000         1376              5        4   \n",
       "5   0.0863167  100000  100000         2176              5        4   \n",
       "6   0.0853191  100000  100000          976              5        4   \n",
       "7   0.0786247  100000  100000         2176              5        4   \n",
       "8   0.0785352  100000  100000         2176              5        4   \n",
       "9   0.0748281  100000  100000         1376              5        4   \n",
       "10  0.0743384  100000  100000         2176              5        4   \n",
       "11  0.0735996  100000  100000         1376              5        4   \n",
       "12  0.0685182  100000  100000         2176              5        4   \n",
       "13  0.0680857  100000  100000         1376              5        4   \n",
       "14  0.0647884  100000  100000         2176              5        4   \n",
       "15  0.0618727  100000  100000         1088              5        4   \n",
       "16  0.0613304  100000  100000         2176              5        4   \n",
       "17  0.0602345  100000  100000         1088              5        4   \n",
       "18  0.0580285  100000  100000          544              5        4   \n",
       "19   0.057791  100000  100000          976              5        4   \n",
       "\n",
       "   latent dimension batchnorm DD_dense   lambda pretrain epochs $\\beta$  \\\n",
       "0                 7      True        2  138.806             300    0.01   \n",
       "1                20      True        1  53.1347             300  0.0001   \n",
       "2                 7      True        1      200              10    0.01   \n",
       "3               200     False        2   20.498              50  0.0001   \n",
       "4                10      True        1  49.0551             200   0.001   \n",
       "5                 3      True        1      200             300     0.1   \n",
       "6                50      True        2  77.6122             200   1e-05   \n",
       "7                 5     False        1  187.761             200     0.1   \n",
       "8                 7      True        1  53.1347              50     0.1   \n",
       "9                20      True        2  175.522             200  0.0001   \n",
       "10                2      True        1  44.9755             100    0.01   \n",
       "11                5      True        2  179.602             300     0.1   \n",
       "12                9     False        2   20.498              50    0.01   \n",
       "13                9     False        1  110.249             200    0.01   \n",
       "14               10      True        2  134.727              50     0.1   \n",
       "15              100     False        2  40.8959              50  0.0001   \n",
       "16              200      True        1  179.602             300  0.0001   \n",
       "17               10      True        1  179.602             200   0.001   \n",
       "18               50     False        1  81.6918             200   0.001   \n",
       "19               20     False        1  167.363             300     0.1   \n",
       "\n",
       "   $\\beta_1$  $\\eta$  none  mse  lrelu  relu  \n",
       "0    0.71793    0.01     1    1      0     1  \n",
       "1   0.615548  0.0001     1    1      0     1  \n",
       "2   0.913043    0.01     1    1      0     1  \n",
       "3   0.598485  0.0001     1    1      0     1  \n",
       "4   0.390532   0.001     1    1      1     0  \n",
       "5   0.488072     0.1     1    1      0     1  \n",
       "6   0.238501   1e-05     1    1      0     1  \n",
       "7   0.692263     0.1     1    1      0     1  \n",
       "8   0.221878     0.1     1    1      0     1  \n",
       "9   0.842695  0.0001     1    1      1     0  \n",
       "10  0.210227    0.01     1    1      1     0  \n",
       "11  0.315545     0.1     1    1      0     1  \n",
       "12  0.612688    0.01     1    1      0     1  \n",
       "13  0.923865    0.01     1    1      1     0  \n",
       "14  0.896138     0.1     1    1      0     1  \n",
       "15  0.302891  0.0001     1    1      0     1  \n",
       "16   0.78403  0.0001     1    1      1     0  \n",
       "17  0.250402   0.001     1    1      1     0  \n",
       "18  0.423839   0.001     1    1      1     0  \n",
       "19  0.855116     0.1     1    1      0     1  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "test_df = test_df.dropna()\n",
    "test_df[\"const\"] = 1\n",
    "data_cols = [c  for c in test_df.columns if c != \"f1_score\"]\n",
    "#data = test_df[data_cols]\n",
    "#print(test_df.f1_score.values.shape)\n",
    "ols_model = sm.OLS(endog=test_df.f1_score.values.astype(float), exog=test_df[data_cols].values.astype(float))\n",
    "result = ols_model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "test_df = test_df.astype(float)\n",
    "corr_array = np.zeros((len(test_df.columns), 2))\n",
    "for i, c in enumerate(test_df.columns):\n",
    "    w, p = spearmanr(test_df[c], test_df[\"ARS\"])\n",
    "    corr_array[i] = [w, p]\n",
    "\n",
    "corr_m = pd.DataFrame(corr_array, columns=[r\"$\\rho_s$\", \"p\"], index=test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$\\rho_s$</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ARS</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$L_x$</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$L_z$</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N parameters</th>\n",
       "      <td>0.373566</td>\n",
       "      <td>0.104711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>largest kernel</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N layers</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latent dimension</th>\n",
       "      <td>-0.284423</td>\n",
       "      <td>0.224224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batchnorm</th>\n",
       "      <td>0.372681</td>\n",
       "      <td>0.105601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DD_dense</th>\n",
       "      <td>0.106199</td>\n",
       "      <td>0.655873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda</th>\n",
       "      <td>-0.036939</td>\n",
       "      <td>0.877126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pretrain epochs</th>\n",
       "      <td>-0.014137</td>\n",
       "      <td>0.952831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$\\beta$</th>\n",
       "      <td>-0.097682</td>\n",
       "      <td>0.682024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$\\beta_1$</th>\n",
       "      <td>0.004511</td>\n",
       "      <td>0.984940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$\\eta$</th>\n",
       "      <td>-0.097682</td>\n",
       "      <td>0.682024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>none</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mse</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lrelu</th>\n",
       "      <td>-0.372681</td>\n",
       "      <td>0.105601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relu</th>\n",
       "      <td>0.372681</td>\n",
       "      <td>0.105601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  $\\rho_s$         p\n",
       "ARS               1.000000  0.000000\n",
       "$L_x$                  NaN       NaN\n",
       "$L_z$                  NaN       NaN\n",
       "N parameters      0.373566  0.104711\n",
       "largest kernel         NaN       NaN\n",
       "N layers               NaN       NaN\n",
       "latent dimension -0.284423  0.224224\n",
       "batchnorm         0.372681  0.105601\n",
       "DD_dense          0.106199  0.655873\n",
       "lambda           -0.036939  0.877126\n",
       "pretrain epochs  -0.014137  0.952831\n",
       "$\\beta$          -0.097682  0.682024\n",
       "$\\beta_1$         0.004511  0.984940\n",
       "$\\eta$           -0.097682  0.682024\n",
       "none                   NaN       NaN\n",
       "mse                    NaN       NaN\n",
       "lrelu            -0.372681  0.105601\n",
       "relu              0.372681  0.105601"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corr_m.to_latex(float_format=lambda x: \"{:.2g}\".format(x),  escape=False),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_vals[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 score</th>\n",
       "      <th>$L_x$</th>\n",
       "      <th>$L_z$</th>\n",
       "      <th>N parameters</th>\n",
       "      <th>largest kernel</th>\n",
       "      <th>N layers</th>\n",
       "      <th>latent dimension</th>\n",
       "      <th>latent loss</th>\n",
       "      <th>reconstruction loss</th>\n",
       "      <th>activation function</th>\n",
       "      <th>batchnorm</th>\n",
       "      <th>$\\beta$</th>\n",
       "      <th>$\\beta_1$</th>\n",
       "      <th>$\\eta$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.815723</td>\n",
       "      <td>0.00423965</td>\n",
       "      <td>0.00336515</td>\n",
       "      <td>976</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>mmd</td>\n",
       "      <td>mse</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.748845</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.80052</td>\n",
       "      <td>0.00412994</td>\n",
       "      <td>100000</td>\n",
       "      <td>976</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>none</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.931548</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.793912</td>\n",
       "      <td>0.00432783</td>\n",
       "      <td>100000</td>\n",
       "      <td>688</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>none</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>True</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.77156</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.797672</td>\n",
       "      <td>0.00417409</td>\n",
       "      <td>0.00729864</td>\n",
       "      <td>2176</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>mmd</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>True</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.441553</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.788598</td>\n",
       "      <td>0.0045926</td>\n",
       "      <td>100000</td>\n",
       "      <td>688</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>none</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.255875</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.780091</td>\n",
       "      <td>0.00512153</td>\n",
       "      <td>100000</td>\n",
       "      <td>1088</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>none</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>False</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.748502</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.78553</td>\n",
       "      <td>0.00570947</td>\n",
       "      <td>0.0241054</td>\n",
       "      <td>1376</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>mmd</td>\n",
       "      <td>mse</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.647847</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.774756</td>\n",
       "      <td>0.0057179</td>\n",
       "      <td>0.0207915</td>\n",
       "      <td>1088</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>mmd</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.374445</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.770226</td>\n",
       "      <td>0.00513076</td>\n",
       "      <td>100000</td>\n",
       "      <td>1088</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>none</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>True</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.768986</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.763657</td>\n",
       "      <td>0.00531745</td>\n",
       "      <td>0.0141592</td>\n",
       "      <td>2176</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>mmd</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>True</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.525585</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.763907</td>\n",
       "      <td>0.00541963</td>\n",
       "      <td>100000</td>\n",
       "      <td>688</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>none</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.331976</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.755668</td>\n",
       "      <td>0.00410553</td>\n",
       "      <td>0.00107543</td>\n",
       "      <td>2176</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>mmd</td>\n",
       "      <td>mse</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.271491</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.757859</td>\n",
       "      <td>0.00453684</td>\n",
       "      <td>0.00236235</td>\n",
       "      <td>1376</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>mmd</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>True</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.598483</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.751815</td>\n",
       "      <td>0.0052313</td>\n",
       "      <td>0.0261567</td>\n",
       "      <td>688</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>mmd</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.38958</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.752279</td>\n",
       "      <td>0.0052577</td>\n",
       "      <td>100000</td>\n",
       "      <td>1376</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>none</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.201815</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.745552</td>\n",
       "      <td>0.00532981</td>\n",
       "      <td>0.0454471</td>\n",
       "      <td>2176</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>mmd</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.425306</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.748109</td>\n",
       "      <td>0.00524046</td>\n",
       "      <td>100000</td>\n",
       "      <td>544</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>none</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>True</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.476656</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.737225</td>\n",
       "      <td>0.541495</td>\n",
       "      <td>100000</td>\n",
       "      <td>2176</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>none</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.948911</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.745834</td>\n",
       "      <td>0.00480458</td>\n",
       "      <td>100000</td>\n",
       "      <td>976</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>none</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.645998</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.732279</td>\n",
       "      <td>0.00598124</td>\n",
       "      <td>100000</td>\n",
       "      <td>2176</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>none</td>\n",
       "      <td>mse</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.855601</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    F1 score       $L_x$       $L_z$ N parameters largest kernel N layers  \\\n",
       "0   0.815723  0.00423965  0.00336515          976              5        4   \n",
       "1    0.80052  0.00412994      100000          976              5        4   \n",
       "2   0.793912  0.00432783      100000          688              5        4   \n",
       "3   0.797672  0.00417409  0.00729864         2176              5        4   \n",
       "4   0.788598   0.0045926      100000          688              5        4   \n",
       "5   0.780091  0.00512153      100000         1088              5        4   \n",
       "6    0.78553  0.00570947   0.0241054         1376              5        4   \n",
       "7   0.774756   0.0057179   0.0207915         1088              5        4   \n",
       "8   0.770226  0.00513076      100000         1088              5        4   \n",
       "9   0.763657  0.00531745   0.0141592         2176              5        4   \n",
       "10  0.763907  0.00541963      100000          688              5        4   \n",
       "11  0.755668  0.00410553  0.00107543         2176              5        4   \n",
       "12  0.757859  0.00453684  0.00236235         1376              5        4   \n",
       "13  0.751815   0.0052313   0.0261567          688              5        4   \n",
       "14  0.752279   0.0052577      100000         1376              5        4   \n",
       "15  0.745552  0.00532981   0.0454471         2176              5        4   \n",
       "16  0.748109  0.00524046      100000          544              5        4   \n",
       "17  0.737225    0.541495      100000         2176              5        4   \n",
       "18  0.745834  0.00480458      100000          976              5        4   \n",
       "19  0.732279  0.00598124      100000         2176              5        4   \n",
       "\n",
       "   latent dimension latent loss reconstruction loss activation function  \\\n",
       "0               200         mmd                 mse                relu   \n",
       "1               100        none                 mse               lrelu   \n",
       "2               100        none                 mse               lrelu   \n",
       "3                50         mmd                 mse               lrelu   \n",
       "4               200        none                 mse               lrelu   \n",
       "5               150        none                 mse               lrelu   \n",
       "6                50         mmd                 mse                relu   \n",
       "7                10         mmd                 mse               lrelu   \n",
       "8               150        none                 mse               lrelu   \n",
       "9                20         mmd                 mse               lrelu   \n",
       "10               10        none                 mse               lrelu   \n",
       "11               50         mmd                 mse                relu   \n",
       "12               20         mmd                 mse               lrelu   \n",
       "13              200         mmd                 mse               lrelu   \n",
       "14               20        none                 mse               lrelu   \n",
       "15              100         mmd                 mse               lrelu   \n",
       "16               20        none                 mse               lrelu   \n",
       "17              150        none                 mse               lrelu   \n",
       "18               50        none                 mse               lrelu   \n",
       "19               10        none                 mse                relu   \n",
       "\n",
       "   batchnorm $\\beta$ $\\beta_1$  $\\eta$  \n",
       "0      False  0.0001  0.748845  0.0001  \n",
       "1       True  0.0001  0.931548  0.0001  \n",
       "2       True   0.001   0.77156   0.001  \n",
       "3       True   0.001  0.441553   0.001  \n",
       "4      False  0.0001  0.255875  0.0001  \n",
       "5      False   1e-05  0.748502   1e-05  \n",
       "6      False  0.0001  0.647847  0.0001  \n",
       "7      False   0.001  0.374445   0.001  \n",
       "8       True   1e-05  0.768986   1e-05  \n",
       "9       True   0.001  0.525585   0.001  \n",
       "10     False   0.001  0.331976   0.001  \n",
       "11      True   0.001  0.271491   0.001  \n",
       "12      True   0.001  0.598483   0.001  \n",
       "13     False  0.0001   0.38958  0.0001  \n",
       "14      True  0.0001  0.201815  0.0001  \n",
       "15      True  0.0001  0.425306  0.0001  \n",
       "16      True   0.001  0.476656   0.001  \n",
       "17      True     0.1  0.948911     0.1  \n",
       "18      True  0.0001  0.645998  0.0001  \n",
       "19      True   1e-05  0.855601   1e-05  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllllllllllll}\n",
      "\\toprule\n",
      "      F1 score &             $L_x$ &             $L_z$ & N parameters & largest kernel & N layers & latent dimension & latent loss & reconstruction loss & activation function & batchnorm &          $\\beta$ &      $\\beta_1$ &           $\\eta$ \\\\\n",
      "\\midrule\n",
      "$\\num{ 0.99 }$ & $\\num{ 0.00032 }$ &   $\\num{ 1e+05 }$ &         4754 &             17 &        3 &              150 &        none &                 mse &                relu &     False &  $\\num{ 1e-05 }$ & $\\num{ 0.73 }$ &  $\\num{ 1e-05 }$ \\\\\n",
      "$\\num{ 0.99 }$ & $\\num{ 1.1e+03 }$ &      $\\num{ 34 }$ &         3798 &             13 &        6 &               50 &         mmd &                 bce &               lrelu &     False &  $\\num{ 0.001 }$ & $\\num{ 0.82 }$ &  $\\num{ 0.001 }$ \\\\\n",
      "$\\num{ 0.98 }$ & $\\num{ 0.00063 }$ & $\\num{ 0.00028 }$ &         3636 &             15 &        5 &              150 &         mmd &                 mse &               lrelu &     False &  $\\num{ 1e-05 }$ & $\\num{ 0.69 }$ &  $\\num{ 1e-05 }$ \\\\\n",
      "$\\num{ 0.98 }$ & $\\num{ 1.1e+03 }$ &    $\\num{ 0.84 }$ &         2408 &             11 &        3 &                3 &         mmd &                 bce &                relu &      True &    $\\num{ 0.1 }$ & $\\num{ 0.56 }$ &    $\\num{ 0.1 }$ \\\\\n",
      "$\\num{ 0.97 }$ & $\\num{ 1.7e+02 }$ &   $\\num{ 1e+05 }$ &          760 &              7 &        5 &              150 &        none &                 bce &               lrelu &      True &  $\\num{ 1e-05 }$ & $\\num{ 0.25 }$ &  $\\num{ 1e-05 }$ \\\\\n",
      "$\\num{ 0.97 }$ &  $\\num{ 0.0011 }$ &  $\\num{ 0.0038 }$ &         1712 &              7 &        3 &              200 &         mmd &                 mse &                relu &     False &    $\\num{ 0.1 }$ & $\\num{ 0.43 }$ &    $\\num{ 0.1 }$ \\\\\n",
      "$\\num{ 0.95 }$ & $\\num{ 1.6e+02 }$ &      $\\num{ 34 }$ &         1740 &             11 &        5 &               50 &         mmd &                 bce &                relu &     False & $\\num{ 0.0001 }$ & $\\num{ 0.53 }$ & $\\num{ 0.0001 }$ \\\\\n",
      "$\\num{ 0.94 }$ & $\\num{ 0.00076 }$ & $\\num{ 0.00075 }$ &          464 &              7 &        5 &              100 &         mmd &                 mse &                relu &     False & $\\num{ 0.0001 }$ & $\\num{ 0.69 }$ & $\\num{ 0.0001 }$ \\\\\n",
      "$\\num{ 0.94 }$ &  $\\num{ 0.0011 }$ &   $\\num{ 1e+05 }$ &         5194 &             15 &        4 &               50 &        none &                 mse &                relu &      True &    $\\num{ 0.1 }$ & $\\num{ 0.93 }$ &    $\\num{ 0.1 }$ \\\\\n",
      "$\\num{ 0.92 }$ & $\\num{ 1.9e+02 }$ &   $\\num{ 1e+05 }$ &         9266 &             17 &        5 &               10 &        none &                 bce &                relu &      True &  $\\num{ 1e-05 }$ & $\\num{ 0.32 }$ &  $\\num{ 1e-05 }$ \\\\\n",
      "$\\num{ 0.92 }$ & $\\num{ 2.5e+02 }$ &   $\\num{ 0.034 }$ &          272 &              5 &        5 &               50 &         mmd &                 bce &                relu &     False &  $\\num{ 0.001 }$ & $\\num{ 0.46 }$ &  $\\num{ 0.001 }$ \\\\\n",
      "$\\num{ 0.91 }$ & $\\num{ 3.2e+02 }$ & $\\num{ 1.5e+02 }$ &         4770 &             11 &        5 &              200 &         kld &                 bce &                relu &      True &   $\\num{ 0.01 }$ & $\\num{ 0.67 }$ &   $\\num{ 0.01 }$ \\\\\n",
      "$\\num{ 0.91 }$ &  $\\num{ 0.0003 }$ &    $\\num{ 0.01 }$ &          688 &              5 &        4 &              150 &         mmd &                 mse &                relu &      True &  $\\num{ 0.001 }$ & $\\num{ 0.62 }$ &  $\\num{ 0.001 }$ \\\\\n",
      " $\\num{ 0.9 }$ & $\\num{ 1.5e+02 }$ &   $\\num{ 1e+05 }$ &         1524 &             11 &        4 &               10 &        none &                 bce &               lrelu &      True &  $\\num{ 0.001 }$ & $\\num{ 0.51 }$ &  $\\num{ 0.001 }$ \\\\\n",
      " $\\num{ 0.9 }$ & $\\num{ 0.00033 }$ &  $\\num{ 0.0013 }$ &         7346 &             17 &        4 &              150 &         mmd &                 mse &                relu &      True & $\\num{ 0.0001 }$ & $\\num{ 0.71 }$ & $\\num{ 0.0001 }$ \\\\\n",
      "$\\num{ 0.89 }$ & $\\num{ 1.1e+03 }$ &   $\\num{ 0.034 }$ &          688 &              5 &        3 &               50 &         mmd &                 bce &               lrelu &     False &    $\\num{ 0.1 }$ & $\\num{ 0.81 }$ &    $\\num{ 0.1 }$ \\\\\n",
      "$\\num{ 0.88 }$ & $\\num{ 3.8e+02 }$ & $\\num{ 9.1e-06 }$ &         3676 &             15 &        4 &               20 &         kld &                 bce &               lrelu &     False & $\\num{ 0.0001 }$ & $\\num{ 0.28 }$ & $\\num{ 0.0001 }$ \\\\\n",
      "$\\num{ 0.83 }$ & $\\num{ 0.00079 }$ &  $\\num{ 0.0092 }$ &          562 &              7 &        6 &              200 &         mmd &                 mse &               lrelu &     False &  $\\num{ 0.001 }$ & $\\num{ 0.37 }$ &  $\\num{ 0.001 }$ \\\\\n",
      "$\\num{ 0.81 }$ & $\\num{ 7.3e+02 }$ &   $\\num{ 1e+05 }$ &         7316 &             11 &        6 &               20 &         kld &                 bce &                relu &     False &    $\\num{ 0.1 }$ & $\\num{ 0.63 }$ &    $\\num{ 0.1 }$ \\\\\n",
      " $\\num{ 0.8 }$ & $\\num{ 1.1e+03 }$ &   $\\num{ 1e+05 }$ &         3546 &             17 &        5 &               50 &        none &                 bce &               lrelu &     False & $\\num{ 0.0001 }$ & $\\num{ 0.85 }$ & $\\num{ 0.0001 }$ \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataframes[0].to_latex(\n",
    "    #\"../tables/randomsearch_convae_simulated_clf/hyperparams.tex\",\n",
    "    index=False,\n",
    "    longtable=False,\n",
    "    float_format=lambda x: r\"$\\num{{ {:.2g} }}$\".format(x),\n",
    "    escape=False,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../metrics_clf/simulated/f1_scores2000.tex\") as fo:\n",
    "    s = \"\".join(fo.readlines())\n",
    "print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
