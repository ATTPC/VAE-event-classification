{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "from mixae import mixae_model, entropy_callback, probabilities_log\n",
    "from batchmanager import BatchManager\n",
    "import data_loader as dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import adjusted_rand_score, confusion_matrix\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from sklearn.utils.fixes import comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy. Requires scikit-learn installed\n",
    "    # Arguments\n",
    "        y: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "    # Return\n",
    "        accuracy, in [0,1]\n",
    "    copied from: https://github.com/XifengGuo/DCEC/blob/master/metrics.py\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "    ind = linear_assignment(w.max() - w)\n",
    "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size\n",
    "\n",
    "def plot_confusion_matrix(\n",
    "        validation_targets,\n",
    "        predicted_values,                      \n",
    "        title = \"\",\n",
    "        true_cols = None,\n",
    "        ax = None \n",
    "        ):\n",
    "    cm_int= confusion_matrix(validation_targets, predicted_values)\n",
    "    \n",
    "    if type(ax) == type(None):\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.set_title(title, fontsize=20)\n",
    "    sns.heatmap(cm_int, annot = True, ax = ax)\n",
    "    if not true_cols is None:\n",
    "        ax.set_yticklabels(true_cols)\n",
    "    ax.set_ylabel('True label', fontsize=14)\n",
    "    ax.set_xlabel('Predicted label', fontsize=14)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    if type(ax) == type(None):\n",
    "        return fig, ax \n",
    "\n",
    "\n",
    "def display_events(model, n, n_display=3, what_class=0,):\n",
    "    if what_class == 0:\n",
    "        event = \"Proton\"  \n",
    "    elif what_class == 1:\n",
    "        event = \"Carbon\"\n",
    "    else:\n",
    "        event = \"Other\" \n",
    "    all_class = np.argwhere(y_lab == what_class)\n",
    "    which_class = [int(all_class[i]) for i in range(n_display)]\n",
    "    print(which_class)\n",
    "    pred = model.predict_on_batch(x_lab_flat[tuple(which_class), :])\n",
    "    lab_samples = pred[0]\n",
    "    lab_probs = pred[1]\n",
    "\n",
    "    cmap = matplotlib.cm.magma\n",
    "    cmap.set_under(color='white')\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=n_classes+1, nrows=n_display, figsize=(15, 9))\n",
    "    ax[0,0].set_title(event+\" events\", fontsize=20)\n",
    "    for i in range(n_display):\n",
    "        w = which_class[i]\n",
    "        ax[i][0].imshow(x_lab_flat[w].reshape(img_shape), cmap=cmap, vmin=1e-4)\n",
    "        ax[i][0].axis(\"off\")\n",
    "        for j in range(n_classes):\n",
    "            ax[i][j+1].imshow(lab_samples[j][i].reshape(img_shape), cmap=cmap, vmin=1e-5)\n",
    "            title = r\"$p_{}\".format(j)\n",
    "            title += r\"= {:.3f}$\".format(float(lab_probs[i,j]))\n",
    "            ax[i][j+1].set_title(title, fontsize=14)\n",
    "            ax[i][j+1].axis(\"off\")\n",
    "    base_fn = \"/home/robersol/github/thesis/chapters/results/clustering/plots/\"\n",
    "    fn = base_fn + event + \"_\"+data+\"_mixae_reconst_{}\".format(n)\n",
    "    plt.savefig(fn+\".pdf\")\n",
    "    plt.savefig(fn+\".png\")\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python3.6/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "(x_full, y_tr), (x_labeled, y_lab) = tf.keras.datasets.mnist.load_data()\n",
    "x_full = (np.expand_dims(x_full, -1)/255)\n",
    "x_labeled = np.expand_dims(x_labeled, -1)/255\n",
    "y_lab_onehot = OneHotEncoder().fit_transform(y_lab.reshape(-1, 1))\n",
    "n_samples = x_full.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "fig, axs = plt.subplots(ncols=4, figsize=(14, 5))\n",
    "og_events = [x_full[i].reshape(x_full[i].shape[0], x_full[i].shape[1]) for i in range(4)]\n",
    "[axs[i].imshow(og_events[i], cmap=\"Greys\") for i in range(4)]\n",
    "[axs[i].axis(\"off\") for i in range(4)]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(y_lab))\n",
    "img_shape = x_full.shape[1:] if len(x_full.shape) == 3 else  x_full.shape[1:-1]\n",
    "\n",
    "n_layers = 3\n",
    "latent_dim = 8\n",
    "kernel_architecture = [3, 3, 3]\n",
    "filter_architecture = [64, 32, 16,]\n",
    "strides_architecture = [2,]*n_layers\n",
    "pooling_architecture = [0,]*n_layers\n",
    "\n",
    "mode_config = {\n",
    "    \"simulated_mode\":False, #deprecated, to be removed\n",
    "    \"restore_mode\":False, #indicates whether to load weights \n",
    "    \"include_KL\":False, #whether to compute the KL loss over the latent space\n",
    "    \"include_MMD\":False, #same as above, but MMD \n",
    "    \"include_KM\":False, #same as above, but K-means. See thesis for a more in-depth treatment of these\n",
    "    \"batchnorm\":False, #whether to include batch-normalization between layers\n",
    "    \"use_vgg\":False, #whether the input data is from a pre-trained model \n",
    "    \"use_dd\":False, #whether to use the dueling-decoder objective \n",
    "}\n",
    "\n",
    "reg_strength = 1e-6\n",
    "ae_args = [\n",
    "    [\n",
    "        n_layers,\n",
    "        filter_architecture,\n",
    "        kernel_architecture,\n",
    "        strides_architecture,\n",
    "        pooling_architecture,\n",
    "        latent_dim,\n",
    "        x_full.shape,\n",
    "    ],\n",
    "    {\n",
    "        \"mode_config\":mode_config\n",
    "    },\n",
    "    [],\n",
    "    {\n",
    "        \"kernel_reg_strength\":reg_strength,\n",
    "        \"kernel_reg\": tf.keras.regularizers.l2,\n",
    "        \"activation\": \"lrelu\"\n",
    "        #\"output_activation\": \"sigmoid\"\n",
    "    }\n",
    "]\n",
    "\n",
    "x_full_flat = x_full.reshape(n_samples, -1)\n",
    "x_lab_flat = x_labeled.reshape(x_labeled.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping = tf.keras.callbacks.EarlyStopping(\n",
    "    \"loss\",\n",
    "    min_delta=1e-2,\n",
    "    patience=2,\n",
    "    #restore_best_weights=True,\n",
    "    #baseline=0.30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_full_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL O (?, 28, 28, 1)\n",
      "FINAL O (?, 28, 28, 1)\n",
      "FINAL O (?, 28, 28, 1)\n",
      "FINAL O (?, 28, 28, 1)\n",
      "FINAL O (?, 28, 28, 1)\n",
      "FINAL O (?, 28, 28, 1)\n",
      "FINAL O (?, 28, 28, 1)\n",
      "FINAL O (?, 28, 28, 1)\n",
      "FINAL O (?, 28, 28, 1)\n",
      "FINAL O (?, 28, 28, 1)\n",
      "target (?, ?, ?)\n",
      "recons (?, ?, ?)\n",
      "mse_pre_sum (10, ?, 784)\n",
      "mse reduced (10, ?)\n",
      "probs  (?, 10)\n",
      "weighted (?, 10)\n",
      "clf ent\n",
      "batch_ent\n",
      "target (?, ?, ?)\n",
      "recons (?, ?, ?)\n",
      "mse_pre_sum (10, ?, 784)\n",
      "mse reduced (10, ?)\n",
      "probs  (?, 10)\n",
      "weighted (?, 10)\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 38s 637us/step - loss: 36.8731 - reconstructions_loss: 0.0685 - soft_prob_loss: 26.2735 - batch_ent_loss: 0.3137\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 24s 401us/step - loss: 33.8426 - reconstructions_loss: 0.0293 - soft_prob_loss: 8.7890 - batch_ent_loss: 0.3150\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 24s 398us/step - loss: 33.3995 - reconstructions_loss: 0.0236 - soft_prob_loss: 3.9780 - batch_ent_loss: 0.3153\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 24s 394us/step - loss: 33.2182 - reconstructions_loss: 0.0212 - soft_prob_loss: 2.4439 - batch_ent_loss: 0.3154\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 23s 390us/step - loss: 33.0897 - reconstructions_loss: 0.0196 - soft_prob_loss: 2.0099 - batch_ent_loss: 0.3153\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 23s 392us/step - loss: 33.0074 - reconstructions_loss: 0.0185 - soft_prob_loss: 1.6518 - batch_ent_loss: 0.3154\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 24s 396us/step - loss: 32.9481 - reconstructions_loss: 0.0178 - soft_prob_loss: 1.3638 - batch_ent_loss: 0.3154\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 23s 390us/step - loss: 32.8797 - reconstructions_loss: 0.0172 - soft_prob_loss: 1.1582 - batch_ent_loss: 0.3152\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 23s 389us/step - loss: 32.8618 - reconstructions_loss: 0.0167 - soft_prob_loss: 1.1113 - batch_ent_loss: 0.3154\n",
      "Epoch 10/100\n",
      "16350/60000 [=======>......................] - ETA: 16s - loss: 32.8562 - reconstructions_loss: 0.0167 - soft_prob_loss: 1.0429 - batch_ent_loss: 0.3154"
     ]
    }
   ],
   "source": [
    "alpha = 0.005\n",
    "beta = 100\n",
    "theta = 10\n",
    "n_clusters = 10\n",
    "\n",
    "mix_obj = mixae_model(ae_args, alpha, beta, x_full_flat.shape[1]/theta)\n",
    "m, mp, clst, _ = mix_obj.compile(n_clusters) \n",
    "pl = probabilities_log(clst, n_clusters, x_lab_flat)\n",
    "ep = 0\n",
    "n_eps = 100\n",
    "unsuper_hist = m.fit(\n",
    "    x_full_flat, \n",
    "    [\n",
    "    np.expand_dims(x_full_flat, 1),\n",
    "    np.zeros(n_samples),\n",
    "    np.zeros(n_samples)\n",
    "    ],\n",
    "    batch_size=150,\n",
    "    epochs=n_eps,\n",
    "    initial_epoch=ep,\n",
    "    callbacks=[\n",
    "        #entropy_callback(mix_obj.alpha, mix_obj.beta),\n",
    "        pl,\n",
    "        earlystopping,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_list = []\n",
    "max_ari = 0.1\n",
    "\n",
    "ari_vals = []\n",
    "acc_vals = []\n",
    "for j, pred in enumerate(pl.prob_log):\n",
    "    #print(pred.shape)\n",
    "    clf_pred = pred.argmax(1)\n",
    "    ari_vals.append(adjusted_rand_score(y_lab, clf_pred))\n",
    "    acc_vals.append(acc(y_lab, clf_pred))\n",
    "perf_list.append([max(ari_vals), max(acc_vals), [alpha, beta, theta]])\n",
    "if max(ari_vals)>max_ari:\n",
    "    print(max(ari_vals), max(acc_vals), [alpha, beta, theta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plot_confusion_matrix(y_lab, pl.prob_log[-1].argmax(1), ax=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (Conda)",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
