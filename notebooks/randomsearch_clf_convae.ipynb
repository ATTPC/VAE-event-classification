{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "viridis = matplotlib.cm.get_cmap('viridis')\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "from evaluate_n_labeled import n_labeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load(\"../data/latent/clf_latent/data_repr.npy\")\n",
    "tar = np.load(\"../data/latent/clf_latent/targets.npy\")\n",
    "#means = [b.mean(0).shape for b in a]\n",
    "#stds = [b.std(0).shape for b in a]\n",
    "#a = [(a[i] - means[i]) for i in range(len(a))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[66.96184539794922, 8230.38671875, 8.458669662475586]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[b.max() for b in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating dataset:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating dataset:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/optimize.py:195: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/usr/local/lib/python3.5/dist-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/optimize.py:195: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/optimize.py:195: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/scipy/optimize/linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/scipy/optimize/linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n"
     ]
    }
   ],
   "source": [
    "means, stds = n_labeled_data(a, tar, 100, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'means' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3ffab8c7deb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'means' is not defined"
     ]
    }
   ],
   "source": [
    "print(means[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 17\n",
      "11 12\n",
      "12 13\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAALlCAYAAADpD74LAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3X+w3fV93/nXGwmBY2EjG0GokBFksOPYaSHcZXBdMtm2xqrdgXTcTXC9W6zUkO4uaTbJeAeSHadLpjtOu9PGO8PUPyjEkzrGrjfNqjELduOkcRtjuGoUx+DIlmWopMWggPjl8kNCn/3jHMHxtSxdSeeecz/nPh4zdzjne75H9/PVEfd5v9/v53xPtdYCACx/p0x7AADA4og2AHRCtAGgE6INAJ0QbQDohGgDQCdEGwA6IdoA0AnRBoBOrJ72ABY666yz2qZNm6Y9DACYmG3btv1Fa239sdZbdtHetGlT5ufnpz0MAJiYqnpoMes5PA4AnRBtAOiEaANAJ0QbADoh2gDQCdEGgE6INgB0QrQBoBOiDQCdEG0A6IRoA0AnRBsAOiHaANAJ0QaATog2AHRCtAGgE6INAJ0QbQDohGgDQCdEGwA6IdoA0AnRBoBOiDYAdEK0AaATog0AnRBtAOiEaANAJ0QbADoh2gDQCdEGgOOw7aH9ueUPdmbbQ/sn/r1XT/w7AkCntj20P//dh/84h1py+qmn5BPvuzyXnr9uYt/fnjYALNI9ux7LoTa4feDgodyz67GJfn/RBoBFuvzC1+aUGtw+dfUpufzC1070+zs8DgCLdOn56/Jv/uFfzT27HsvlF752oofGE9EGgONy6fnrJh7rwxweB4BOiDYAdEK0AaATog0AnRBtAOiEaANAJ0QbgO6940N/lL/261+YyvXAJ0m0Aejatof258+//XT27H8277n1npkOt2gDTNFPf+RL+emPfGnawxi7Se75Tvt64JMk2gCM1aT3fKd9PfBJEm0AxmrSe76Xnr8uP/yDZ+S8da+Y+EdlTpprjwNM0dPPHchTzx3Mtof2z0xsLr/wtTn91FNy4OChie353vnzP77k32M5EG2AKTl8GPlQS95z6z0zs5d46fnr8on3XT61T8KaZaINsMDhiWGf+tm3LOn3OdJh5FkJ3DQ/CWuWOacNMCWTnkA1qzPVVxLRBpiSSU+gevq5A9n7xLMz/T7mWSfaAFN0xumnZsOZr1jyYK+kC5DMMtEGWAFW0gVIZploA11wPvbkrKQLkMyyRUW7qjZX1Y6q2llVNx7h8ddV1R9U1Z9U1Veq6h0jj900fN6Oqnr7OAcP0LtP/exblnyWerKyLkAyy475lq+qWpXkliRvS7InyX1VtbW19sDIav9bkk+31v5lVf1IkjuTbBrevibJm5L8pST/vqpe31p7cdwbAsDRnXH6qTnj9FMFu2OLeZ/2ZUl2ttZ2JUlV3ZHk6iSj0W5JXjW8/eok/9/w9tVJ7mitPZ/kW1W1c/jnOcYFMGGT2KNnaS3m8PiGJLtH7u8ZLhv1j5P891W1J4O97J87juemqq6vqvmqmt+3b98ihw4AK8u4JqK9O8lvttbOS/KOJL9VVYv+s1trH22tzbXW5tavXz+mIcHyMbOTqG5/5+Brxng/M8vVYsK6N8nGkfvnDZeN+gdJPp0krbUvJTk9yVmLfC7AsuH9zCxni4n2fUkuqqoLqmpNBhPLti5Y578k+RtJUlVvzCDa+4brXVNVp1XVBUkuSnLvuAYPMG7ez8xydsyJaK21g1V1Q5K7k6xKcltr7f6qujnJfGtta5JfSvKxqvqFDCalvbe11pLcX1WfzmDS2sEk/7OZ47C0JvVhF7Pq8PuZDzXvZ2b5WdSnfLXW7sxggtnosg+M3H4gyVu/z3P/SZJ/chJjBJiYw+9nfuq5g/nQNZd4exTLiiuiAV2Y5OSwSV0PHI6XaAMn7P6Hn8z9Dz+55N/H5DAYEG1g2TM5DAZEG1j2fNgFDCxqIhrANJkcBgP2tIET9rWDG/Lp599ichhMiGgDJ2TbQ/vzy//13fmt53/c5DCYENEGTsg9ux7LwazKoZxichhMiGgDJ+TyC1+b1Xkxp+RFk8NgQkxEA07Ipeevy//xA5/Mn734ulz1M788U+eaXQKW5Uq0gRP2xtV788bVe/OmGQo2LGeizYo1qx+s8YHH3j+89R+nOg5g/JzThhkzybdhAZNlTxtmyOG3YR3Mqnzq1nvyifddPjPnmmftiAicCHvaLCs//ZEvvXTYepZM6hOqJv02rDed++q86dxXL+n3AF4m2p2a1bjNokl+QtXE34b1/FPJk7uT3fcu7fcBkog2LLlJfkLV4bdh/Q+nfXHpD43vvjd55KvJEw8lH79KuGECRBuW2KQ/oeqNq/fmp0770tKfy37wi0k7NLj94guD+8CSEm1WrEmdZz78CVXnrXvFTE0My6Yrkhr+CFm1ZnAfWFKizYo0yfPMyWQ/oeoVh76Ts158dOkPV2+8LDnnzcmZ5yfXbh3cB5aUaLMiTfI880TtvjebDn4rZ7/4yGTOM5/2quTVGwUbJkS0WZEmfZ55Yh78YiotlTjPDDPIxVVYkQ6fZ37quYP50DWXzNR55kGyW8p5Zpg5os2Kdcbpp+aM00+dnWAnycbL8uDqC/LK9p2cc+2/dtgaZoxow4x59pRX5tm8MucINswc57RZVib1NiyAHok2y8ak34YF0BvR5pgmdZ3zmX0bFsCYiDbLxsy+DQtgTExEY9mY2bdhTZiPyoTZJdosK5N8G9anfvYtS/49GKPb3zn475bPTnccMEWiPUaHz/tOIgZPP3cgTz13MNse2r/kgZvk9/rAY+8f3vqPS/p9Zpqonbzd9w6uJrfpCu91Z1lxTrtDk5xlbUY3K87ue5Pb3p78/s0+J5xlR7Q7NMlZ1mZ0s+L4nHCWMdHu0CRnWZvRzYrjc8JZxpzT7tAkZ1mb0c2Ks/Gy5Gfudk6bZUm0OzXJWdYz+cEaE2aC3Rg8/1Ty3JODc8xLHdKNl4k1y5LD4zABb3pN5U0/8JRJTSdq973JI19NnnjI5DBWNHvaHJP3M5+kw8FphwbBuXbr7OzFTertZUeaHDYrf4dwHOxpw1IzG/nkmRwGSexps8zM5CU4DwenHRKcE7XxsuScNw/Oab/rVnvZrFiizfIyyclGk2I28nic9qrBl78/VjDRZvmY5XO/ZiMDY+CcNsuHc78AR2VPe4x8sMZJcu4X4KjsaY+JD9YYg8OTjc48f7YOjQOMiWiPyUx/sMbt73z5s4yX2mmvSl69UbABjsDh8TE5/MEah5oP1oAl4XPCYfaj/dMf+VKSpb+qlw/W6NDue70NC+jKzEd7knywRkd235vc9vbBpLfVr3AOHeiCc9qsTA9+Mcnwg8K9vQzohGizMm26YvC2slrl7WVANxwe79RMXqN7kjZeNjgk7pw20BHRZuVyaVGgMw6PA0AnRBsAOuHwOMuLC2gAfF/2tAGgE6INAJ0QbQDohGgDQCdMRBujpf5QEgBWNnvaANAJ0QaATog2AHRCtAGgE6LNsT3/VPLk7mT3vdMeCcCKJtoc3e57k0e+mjzxUPLxq4QbYIpEm6N78ItJOzS4/eILg/sATIVo92pSh6w3XZHU8J/JqjWD+wBMhWj3aJKHrDdelpzz5uTM85Nrtw7uAzAVot2jSR+yPu1Vyas3CjbAlIl2jxyyBliRXHu8R4cPWT/3ZPKuW+0BA6wQot2r0141+BJsgBXD4XEA6IRoA0AnRBsAOiHaANAJ0QaATog2AHRCtAGgE4uKdlVtrqodVbWzqm48wuP/oqq2D7++XlVPjDz24shjW8c5eABYSY55cZWqWpXkliRvS7InyX1VtbW19sDhdVprvzCy/s8luWTkj3i2tXbx+IYMACvTYva0L0uys7W2q7X2QpI7klx9lPXfneST4xgcAPCyxUR7Q5LdI/f3DJd9j6o6P8kFSb4wsvj0qpqvqnuq6ie/z/OuH64zv2/fvkUOHQBWlnFPRLsmyWdaay+OLDu/tTaX5O8l+Y2q+qGFT2qtfbS1Ntdam1u/fv1YB/T0cwey94lns+2h/WP9cwFg0hYT7b1JNo7cP2+47EiuyYJD4621vcP/7kryh/nu891LattD+/Pn3346e/Y/m/fceo9wA9C1xUT7viQXVdUFVbUmgzB/zyzwqvrhJOuSfGlk2bqqOm14+6wkb03ywMLnLpV7dj2WQ21w+8DBQ7ln12OT+tYAMHbHnD3eWjtYVTckuTvJqiS3tdbur6qbk8y31g4H/Jokd7TW2sjT35jkI1V1KINfED44Out8qV1+4WtzSiWHWnLq6lNy+YWvndS3ni1bPjvtEQCQRX6edmvtziR3Llj2gQX3//ERnvfHSX70JMZ3Ui49f11++AfPyFPPHcyHrrkkl56/blpDAYCTtqho9+yM00/NGaefKtgAdM9lTAGgE6INAJ0QbQDohGgDQCdEGwA6IdoA0AnRBoBOzPz7tGeWq5QBrDj2tAGgE6INAJ0QbQDohGgDQCdEGwA6IdoA0AnRBoBOiDYAdEK0AaATog0AnRBtAOiEaANAJ0QbADoh2gDQCdEGgE6INgB0QrQBoBOiDQCdEG0A6IRoA0AnRBsAOiHaANAJ0QaATog2AHRCtAGgE6INAJ0QbQDohGgDQCdWT3sAS+1TP/uWaQ8BAMbCnjYAdEK0AaATog0AnRBtAOiEaANAJ0QbADoh2gDQCdEGgE6INgB0QrQBoBOiDQCdEG0A6IRoA0AnRBsAOiHaANAJ0QaATog2AHRCtAGgE6INAJ0Q7XG6/Z2DLwBYAqINAJ0QbQDohGgDQCdEGwA6IdoA0AnRBoBOiPY4Pf9U8uTuZPe90x4JADNItMdl973JI19Nnngo+fhVwg3A2In2uDz4xaQdGtx+8YXBfQAYI9Eel01XJDX861y1ZnAfAMZo9bQHMDM2Xpac8+bkuSeTd906uA8AYyTa43TaqwZfgg3AEnB4HAA6IdoA0AnRBoBOiDYAdEK0AaATZo+P05bPTnsEAMwwe9oA0AnRBoBOiDYAdEK0AaATog0AnVhUtKtqc1XtqKqdVXXjER7/F1W1ffj19ap6YuSxa6vqG8Ova8c5eABYSY75lq+qWpXkliRvS7InyX1VtbW19sDhdVprvzCy/s8luWR4+zVJfjXJXJKWZNvwufvHuhUAsAIsZk/7siQ7W2u7WmsvJLkjydVHWf/dST45vP32JJ9vrT0+DPXnk2w+mQEDwEq1mGhvSLJ75P6e4bLvUVXnJ7kgyReO97kAwNGNeyLaNUk+01p78XieVFXXV9V8Vc3v27dvzEMCgNmwmGjvTbJx5P55w2VHck1ePjS+6Oe21j7aWptrrc2tX79+EUMCgJVnMdG+L8lFVXVBVa3JIMxbF65UVT+cZF2SL40svjvJlVW1rqrWJblyuAwAOE7HnD3eWjtYVTdkENtVSW5rrd1fVTcnmW+tHQ74NUnuaK21kec+XlW/lkH4k+Tm1trj490EAFgZaqSxy8Lc3Fybn5+f9jAAYGKqaltrbe5Y67kiGgB0QrQBoBOiDQCdEG0A6IRoA0AnRBsAOiHaANAJ0QaATog2AHRCtAGgE6INAJ0QbQDohGgDQCdEGwA6IdoA0AnRBoBOiDYAdEK0AaATog0AnRBtAOiEaANAJ0QbADoh2gDQCdEGgE6INgB0QrQBoBOiDQCdEG0A6IRoA0AnRBsAOiHaANAJ0QaATog2AHRCtAGgE6INAJ0QbQDohGgDQCdEGwA6IdoA0AnRBoBOiDYAdEK0AaATog0AnRBtAOiEaANAJ0QbADoh2gDQCdEGgE6INgB0QrQBoBOiDQCdEG0A6IRoA0AnRBsAOiHaANAJ0QaATog2AHRCtAGgE6INAJ0QbQDohGgDQCdEGwA6IdoA0AnRBoBOiDYAdEK0AaATog0AnRBtAOiEaANAJ0QbADoh2gDQCdEGgE6INgB0QrQBoBOiDQCdEG0A6IRoA0AnRBsAOiHaANAJ0QaATog2AHRCtAGgE4uKdlVtrqodVbWzqm78Puv8VFU9UFX3V9Vvjyx/saq2D7+2jmvgALDSrD7WClW1KsktSd6WZE+S+6pqa2vtgZF1LkpyU5K3ttb2V9XZI3/Es621i8c8bgBYcRazp31Zkp2ttV2ttReS3JHk6gXrXJfkltba/iRprT063mECAIuJ9oYku0fu7xkuG/X6JK+vqv9UVfdU1eaRx06vqvnh8p88yfECwIp1zMPjx/HnXJTkJ5Kcl+SPqupHW2tPJDm/tba3qi5M8oWq+rPW2jdHn1xV1ye5Pkle97rXjWlIADBbFrOnvTfJxpH75w2XjdqTZGtr7UBr7VtJvp5BxNNa2zv8764kf5jkkoXfoLX20dbaXGttbv369ce9EQCwEiwm2vcluaiqLqiqNUmuSbJwFvjvZrCXnao6K4PD5buqal1VnTay/K1JHggAcNyOeXi8tXawqm5IcneSVUlua63dX1U3J5lvrW0dPnZlVT2Q5MUk72+tPVZVfzXJR6rqUAa/IHxwdNY5ALB41Vqb9hi+y9zcXJufn5/2MABgYqpqW2tt7ljruSIaAHRCtAGgE6INAJ0QbQDohGgDQCdEGwA6IdoA0AnRBoBOiDYAdEK0AaATog0AnRBtAOiEaANAJ0QbADoh2gDQCdEGgE6INgB0QrQBoBOiDQCdEG0A6IRoA0AnRBsAOiHaANAJ0QaATog2AHRCtAGgE6INAJ0QbQDohGgDQCdEGwA6IdoA0AnRBoBOiDYAdEK0AaATog0AnRDtMdpy15ZsuWvLtIcBwIwSbQDohGgDQCdEGwA6IdoA0AnRBoBOiDYAdEK0AaATog0AnRBtAOiEaANAJ0QbADox89He8ptz2fKbc9MeBgCctJmPNgDMCtEGgE6INgB0QrQBoBOiDQCdEG0A6IRoA0AnRBsAOiHaANAJ0QaATog2AHRCtAGgE6INAJ2Y+Wj/0HPP5p1PPJ7svnfaQwGAkzLb0d59b97/yN78nSceSz5+lXAD0LXZjvaDX8zq1rIqSV58IXnwi9Me0dhsuWtLtty1ZdrDAGCCZjvam67Iwaq8mCSr1iSbrpj2iADghM12tDdeln92zob82zNfm1y7Ndl42bRHBAAnbLajneSbp78inz3zNYINQPdmPtoAMCtEGwA6IdoA0AnRBoBOiDYAdEK0AaATog0AnRBtAOiEaANAJ0QbADoh2gDQCdEGgE6snvYAltwP/ui0RwAAY2FPGwA6IdoA0IlFRbuqNlfVjqraWVU3fp91fqqqHqiq+6vqt0eWX1tV3xh+XTuugQPASnPMc9pVtSrJLUnelmRPkvuqamtr7YGRdS5KclOSt7bW9lfV2cPlr0nyq0nmkrQk24bP3T/+TQGA2baYPe3Lkuxsre1qrb2Q5I4kVy9Y57oktxyOcWvt0eHytyf5fGvt8eFjn0+yeTxDB4CVZTHR3pBk98j9PcNlo16f5PVV9Z+q6p6q2nwczwUAFmFcb/laneSiJD+R5Lwkf1RVi36vVVVdn+T6JHnd6143piEBwGxZzJ723iQbR+6fN1w2ak+Sra21A621byX5egYRX8xz01r7aGttrrU2t379+uMZPwCsGIuJ9n1JLqqqC6pqTZJrkmxdsM7vZrCXnao6K4PD5buS3J3kyqpaV1Xrklw5XAYAHKdjHh5vrR2sqhsyiO2qJLe11u6vqpuTzLfWtublOD+Q5MUk72+tPZYkVfVrGYQ/SW5urT2+FBsCALNuUee0W2t3JrlzwbIPjNxuSX5x+LXwubclue3khgkAuCIaAHRCtAGgE6INAJ0QbQDohGgDQCdEGwA6IdoA0AnRBoBOiDYAdEK0AaATog0AnRBtAOiEaANAJ0QbADoh2gDQCdEGgE6INgB0QrQBoBOiDQCdEG0A6IRoA0AnRBsAOiHaANAJ0QaATog2AHRCtAGgE6INAJ0QbQDohGgDQCdEGwA6IdoA0AnRBoBOiDYAdEK0AaATog0AnRBtAOiEaANAJ0QbADoh2gDQCdEGgE6INgB0QrQBoBOiDQCdEG0A6IRoA0AnRBsAOiHaANAJ0QaATog2AHRCtAGgE6INAJ0QbQDohGgDQCdEGwA6IdoA0AnRBoBOiDbAAlvu2pItd22Z9jDge4g20AUhBdEGToKQwmSJNgB0QrQBoBOiDQCdEG0A6IRoA0AnRBsAOiHaAAs888Izefg7D2f7o9unPRT4LqINMGL7o9uzY/+O7H1mb6773HXCzbIi2gAj5h+ZT0tLkhw4dCDzj8xPeUTwMtEGGDF3zlwqlSQ59ZRTM3fO3JRHBC9bPe0BACwnF599cd6w7g15+sDT+eAVH8zFZ1887SHBS0QbYIG1a9Zm7Zq1gs2y4/A4AHRCtAGgE6INAJ0QbQDohGgDQCdEG2CKtty1JVvu2jLtYdAJ0QaATog2cMJ8sAZMlourACfk8AdrtLRc97nr8rErPzYzFyO5ffPt0x4CHJE9beCE+GANmDzRBk6ID9aAyVtUtKtqc1XtqKqdVXXjER5/b1Xtq6rtw6/3jTz24sjyreMcPDA9hz9YY8PaDTN1aByWs2Oe066qVUluSfK2JHuS3FdVW1trDyxY9VOttRuO8Ec821pbEf83P/PCM3n6wNPZ/uh2P8BYEXywBkzWYva0L0uys7W2q7X2QpI7kly9tMPqz+FJOXuf2ZvrPned2bQAjN1ior0hye6R+3uGyxZ6V1V9pao+U1UbR5afXlXzVXVPVf3kkb5BVV0/XGd+3759ix/9MmJSDgBLbVwT0f5dkk2ttb+c5PNJPj7y2Pmttbkkfy/Jb1TVDy18cmvto621udba3Pr168c0pMkyKQeWlveEw+KivTfJ6J7zecNlL2mtPdZae35499Ykl448tnf4311J/jDJJScx3mXLpBxYOk4/wcBion1fkouq6oKqWpPkmiTfNQu8qs4duXtVkq8Nl6+rqtOGt89K8tYkCyewzYy1a9bm3FeeO3PBdm1kps3pJxg45uzx1trBqrohyd1JViW5rbV2f1XdnGS+tbY1yT+qqquSHEzyeJL3Dp/+xiQfqapDGfyC8MEjzDoHOKrDp59amtNPrGiLuoxpa+3OJHcuWPaBkds3JbnpCM/74yQ/epJjBFa4w6efnj7wdD54xQdn7mgWLJZrjwNd8J5wcBlTmDnmIMDsEm0A6IRoA0AnRBsAOiHaANCJmY+2Sx8CMCtmOtoufQgsd3YsOB4zHW2XPgSWMzsWHK+ZjrZP3gKWMzsWHK+ZviKaSx8Cy5lrqnO8ZjraiUsfAsuXHQuO18xHG1g6t2++fdpD6J4dC47HTJ/TBoBZItoA0AnRBoBOiHanXJABYOUR7Q65IAPAyiTaHXJBBoCVSbQ75EpvHI1TJzC7RLtDhy/IsGHthnzsyo95fycvceoEZptod2rtmrU595XnCjbfxakTmG2iDTPEqROYbS5jCjPEtaxhtok2zBjXsobZ5fA4AHRCtAGgEw6PA13wMaBgTxsAuiHaANAJ0QaATog2AHRCtFmxtty1JVvu2jLtYQAsmtnjAFNkVjzHw542AHRCtAGgE6INAJ0QbQDohGgDQCdEGwA64S1fMGO8hQhmlz1tAOiEaANAJ0QbADoh2gDQCdEGgE6INgB0QrQBoBOiDQCdEG0A6IRoA0AnRBsAOiHaMAFb7tqSLXdtmfYwgM6JNgB0QrQBoBOiDQCdEG0A6IRoA0AnRBsAOiHaANAJ0QaATog2AHRCtAGgE6LNMT3zwjN5+DsPZ/uj26c9FIAVTbQ5qu2Pbs+O/Tuy95m9ue5z1wn3CfKLDzAOos1RzT8yn5aWJDlw6EDmH5mf8ojGZ1Ih9YsPMC6izVHNnTOXSiVJTj3l1MydMzflEY3HJEM6y7/4AJMl2hzVxWdfnDese0M2rN2Qj135sVx89sXTHtJYTDKks/qLDzB5q6c9AJa/tWvWZu2atTMT7OTlkLa0JQ/p4V98nj7wdD54xQdn6u8RmCzRZkWadEhn8RcfYPJEm2Vly11bkiS3b759yb+XkAK9cU4bADoh2gDQCdEGgE6INgB0QrQBoBOiDQCdEG0A6IRoA0AnRBsAOiHaANAJlzGFCZjEZVmB2beoPe2q2lxVO6pqZ1XdeITH31tV+6pq+/DrfSOPXVtV3xh+XTvOwQPASnLMPe2qWpXkliRvS7InyX1VtbW19sCCVT/VWrthwXNfk+RXk8wlaUm2DZ+7fyyjB4AVZDF72pcl2dla29VaeyHJHUmuXuSf//Ykn2+tPT4M9eeTbD6xobISPPPCM3n4Ow9n+6Pbpz0UgGVnMdHekGT3yP09w2ULvauqvlJVn6mqjcfz3Kq6vqrmq2p+3759ixw6s2b7o9uzY/+O7H1mb6773HXCDbDAuGaP/7skm1prfzmDvemPH8+TW2sfba3Ntdbm1q9fP6Yh0Zv5R+bT0pIkBw4dyPwj81MeEcDyspho702yceT+ecNlL2mtPdZae35499Ykly72uXDY3DlzqVSS5NRTTs3cOXNTHhHA8rKYaN+X5KKquqCq1iS5JsnW0RWq6tyRu1cl+drw9t1JrqyqdVW1LsmVw2XwPS4+++K8Yd0bsmHthnzsyo/l4rMvnvaQAJaVY84eb60drKobMojtqiS3tdbur6qbk8y31rYm+UdVdVWSg0keT/Le4XMfr6pfyyD8SXJza+3xJdgOZsTaNWuzds1awQY4gkVdXKW1dmeSOxcs+8DI7ZuS3PR9nntbkttOYowAQFzGFAC6IdoA0AnRBoBOiDYAdEK0AaATog0AnRBtAOiEaANAJ0QbADoh2gDQCdEGgE6INgB0YlEfGAKTcvvm22fyewGMgz1tAOiEaANAJ0QbADoh2gDQCdEGgE6INgB0QrQBoBOiDQCdEG0A6IRoA0AnRBsAOiHaANAJ0QaATog2AHRCtAGgE6INAJ0QbQDohGgDQCdEGwA6IdoA0AnRBoBOiDYAdEK0AaATog0AnRBtAOiEaANAJ0QbADoh2gDQCdEGgE6snvYAWP5u33z7tIcAQOxpA0A37GmPkT1SAJaSPW0A6IRoA0AnRBsAOiHaANB9vIcXAAAIhUlEQVQJ0QaATog2AHRCtAGgE6INAJ0QbQDohGgDQCdEGwA6IdoA0AnRBoBOiDYAdEK0AaATog0AnVg97QEstds33z7tIQDAWNjTBoBOiDYAdEK0AaATM39Oe1Y5Vw+w8tjTBoBOiDYAdEK0AaATog0AnRBtAOiEaANAJ0QbADoh2gDQCdEGgE6INgB0QrQBoBOiDQCdEG0A6IRoA0AnRBsAOiHaANCJRUW7qjZX1Y6q2llVNx5lvXdVVauqueH9TVX1bFVtH359eFwDB4CVZvWxVqiqVUluSfK2JHuS3FdVW1trDyxY74wkP5/kywv+iG+21i4e03gBYMVazJ72ZUl2ttZ2tdZeSHJHkquPsN6vJfn1JM+NcXwAwNBior0hye6R+3uGy15SVT+WZGNr7bNHeP4FVfUnVfUfquqKI32Dqrq+quaran7fvn2LHTsArCgnPRGtqk5J8s+T/NIRHn44yetaa5ck+cUkv11Vr1q4Umvto621udba3Pr16092SAAwkxYT7b1JNo7cP2+47LAzkrw5yR9W1YNJLk+ytarmWmvPt9YeS5LW2rYk30zy+nEMHABWmsVE+74kF1XVBVW1Jsk1SbYefrC19mRr7azW2qbW2qYk9yS5qrU2X1XrhxPZUlUXJrkoya6xbwUArADHnD3eWjtYVTckuTvJqiS3tdbur6qbk8y31rYe5ek/nuTmqjqQ5FCSf9hae3wcAweAlaZaa9Mew3eZm5tr8/Pz0x4GAExMVW1rrc0daz1XRAOATog2AHRCtAGgE6INAJ0QbQDohGgDQCeW3Vu+qmpfkoemPY6TcFaSv5j2IJaA7eqL7eqL7erPuLft/NbaMa/jveyi3buqml/Me+16Y7v6Yrv6Yrv6M61tc3gcADoh2gDQCdEev49OewBLxHb1xXb1xXb1Zyrb5pw2AHTCnjYAdEK0j0NVbayqP6iqB6rq/qr6+eHy11TV56vqG8P/rhsur6r6v6pqZ1V9pap+bLpbcHRVtaqq/qSqfm94/4Kq+vJw/J8afp56quq04f2dw8c3TXPcR1NVZ1bVZ6rqz6vqa1X1lll4varqF4b/Br9aVZ+sqtN7fb2q6raqerSqvjqy7Lhfo6q6drj+N6rq2mlsy6jvs13/bPhv8StV9W+r6syRx24abteOqnr7yPLNw2U7q+rGSW/HQkfarpHHfqmqWlWdNbzf9es1XP5zw9fs/qr6pyPLp/N6tdZ8LfIryblJfmx4+4wkX0/yI0n+aZIbh8tvTPLrw9vvSPL/Jqkklyf58rS34Rjb94tJfjvJ7w3vfzrJNcPbH07yPw5v/09JPjy8fU2ST0177EfZpo8ned/w9pokZ/b+eiXZkORbSV4x8jq9t9fXK8mPJ/mxJF8dWXZcr1GS1yTZNfzvuuHtdctwu65Msnp4+9dHtutHkvxpktOSXJDkm0lWDb++meTC4b/fP03yI8ttu4bLNya5O4PrbJw1I6/Xf5vk3yc5bXj/7Gm/XlP7C5qFryT/T5K3JdmR5NzhsnOT7Bje/kiSd4+s/9J6y+0ryXlJfj/JX0/ye8P/yf5i5AfMW5LcPbx9d5K3DG+vHq5X096GI2zTqzOIWy1Y3vXrlUG0dw9/4K0evl5v7/n1SrJpwQ/L43qNkrw7yUdGln/XestluxY89neSfGJ4+6YkN408dvfwNXzpdTzSestpu5J8JslfSfJgXo52169XBr8I/80jrDe118vh8RM0PMR4SZIvJzmntfbw8KFvJzlnePvwD9fD9gyXLUe/keR/TXJoeP+1SZ5orR0c3h8d+0vbNXz8yeH6y80FSfYluX142P/WqnplOn+9Wmt7k/yfSf5Lkocz+Pvflv5fr1HH+xp18dot8DMZ7IUmnW9XVV2dZG9r7U8XPNT1diV5fZIrhqeV/kNV/TfD5VPbLtE+AVW1Nsn/neR/aa09NfpYG/x61dWU/Kr620keba1tm/ZYxmx1Boe7/mVr7ZIk38ngUOtLOn291iW5OoNfSv5Sklcm2TzVQS2hHl+jY6mqX0lyMMknpj2Wk1VVP5Dkl5N8YNpjWQKrMziidXmS9yf5dFXVNAck2sepqk7NINifaK39znDxI1V17vDxc5M8Oly+N4PzPIedN1y23Lw1yVVV9WCSOzI4RP6hJGdW1erhOqNjf2m7ho+/OsljkxzwIu1Jsqe19uXh/c9kEPHeX6+/meRbrbV9rbUDSX4ng9ew99dr1PG+Rr28dqmq9yb520neM/yFJOl7u34og18g/3T4M+S8JP+5qn4wfW9XMvgZ8jtt4N4MjkSelSlul2gfh+FvWP8qyddaa/985KGtSQ7Pfrw2g3Pdh5f//eEMysuTPDlyyG/ZaK3d1Fo7r7W2KYOJSl9orb0nyR8k+bvD1RZu1+Ht/bvD9ZfdnlBr7dtJdlfVG4aL/kaSB9L565XBYfHLq+oHhv8mD29X16/XAsf7Gt2d5MqqWjc8EnHlcNmyUlWbMzgNdVVr7b+OPLQ1yTU1mOl/QZKLktyb5L4kF9XgnQFrMvj/c+ukx300rbU/a62d3VrbNPwZsieDCbvfTuevV5LfzWAyWqrq9RlMLvuLTPP1mvaJ/56+kvy1DA7TfSXJ9uHXOzI4P/j7Sb6RwUzD1wzXryS3ZDCb8M+SzE17GxaxjT+Rl2ePXzj8h7gzyb/JyzMoTx/e3zl8/MJpj/so23Nxkvnha/a7GcxU7f71SvK/J/nzJF9N8lsZzGLt8vVK8skMzs0fyOAH/j84kdcog3PEO4dfW5bpdu3M4Jzn4Z8fHx5Z/1eG27Ujyd8aWf6ODN6p8s0kv7Ict2vB4w/m5Ylovb9ea5L86+H/Z/85yV+f9uvlimgA0AmHxwGgE6INAJ0QbQDohGgDQCdEGwA6IdoA0AnRBoBOiDYAdOL/B+4HU+oxkt+DAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x936 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 13))\n",
    "\n",
    "for i in range(len(a)):\n",
    "    x = [100*i for i in range(1, means[i].shape[0])]\n",
    "    print(len(x), len(means[i]))\n",
    "    ax.errorbar(x, means[i][:-1], yerr=stds[i][:-1], fmt=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########\n",
      "0.9866666666666667\n",
      "0.9866666666666667 [list([[2, 16, 64], [17, 15, 3], [2, 2, 2], [0, 0, 0], 3])\n",
      " list([0.01, 1e-05, 0.7267722581866471, 0.99, 150])\n",
      " {'batchnorm': False, 'include_MMD': False, 'include_KL': False, 'include_KM:': False, 'simulated_mode': False, 'restore_mode': False}\n",
      " {} 'mse' 'relu']\n",
      "Max reconst loss:  0.001136922393925488\n",
      "Reconst loss : mse\n",
      "test f1  0.9866666666666667 [0.98666667 0.98666667]\n",
      "Ind of experiment:  13 | run:  96\n",
      "N runs:  43\n",
      "-----------\n",
      "[list([[2, 16, 64], [17, 15, 3], [2, 2, 2], [0, 0, 0], 3])\n",
      " list([0.01, 1e-05, 0.7267722581866471, 0.99, 150])\n",
      " {'batchnorm': False, 'include_MMD': False, 'include_KL': False, 'include_KM:': False, 'simulated_mode': False, 'restore_mode': False}\n",
      " {} 'mse' 'relu']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:61: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../results/randomsearch_convae_cleanevent_128_clf/run_160/hyperparam_vals_static.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b1b9bae7e71d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"simulated\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mparam_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_str\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"run_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/hyperparam_vals_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_str\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"run_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/loss_vals_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mperformance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_str\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"run_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/performance_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../results/randomsearch_convae_cleanevent_128_clf/run_160/hyperparam_vals_static.npy'"
     ]
    }
   ],
   "source": [
    "algo = \"convae\"\n",
    "run =  \"160\"\n",
    "arch = \"static\"\n",
    "#data = \"cleanevent\"\n",
    "size = \"128\"\n",
    "datasets = [\"simulated\", \"cleanevent\", \"realevent\"]\n",
    "runs = [\"96\", \"160\", \"133\"]\n",
    "#datasets = [\"realevent\"]\n",
    "#runs = [\"188\",]\n",
    "#datasets =  [\"realevent\", \"realevent\"]\n",
    "#runs = [\"188\", \"175\"]\n",
    "#datasets = [\"vgg_simulated\", \"vgg_cleanevent\", \"vgg_realevent\"]\n",
    "#runs = [\"165\", \"159\", \"146\"]\n",
    "#datasets = [\"vgg_cleanevent\"]\n",
    "#runs = [\"158\"]\n",
    "losses_all = []\n",
    "performance_all = []\n",
    "params_all = []\n",
    "\n",
    "to_print = [13, 84, 116]\n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    data = datasets[i]\n",
    "    run = runs[i]\n",
    "    base_str = \"../results/randomsearch_\"+algo+\"_\"+data+\"_\"+size+\"_clf/\"\n",
    "    \n",
    "    if data != \"simulated\":\n",
    "        param_vals = np.load(base_str+\"run_\"+run+\"/hyperparam_vals_\"+arch+\".npy\")\n",
    "        losses = np.load(base_str+\"run_\"+run+\"/loss_vals_\"+arch+\".npy\")\n",
    "        performance = np.load(base_str+\"run_\"+run+\"/performance_\"+arch+\".npy\")\n",
    "    else:\n",
    "        param_vals = np.load(base_str+\"run_\"+run+\"hyperparam_vals_ours.npy\")\n",
    "        losses = np.load(base_str+\"run_\"+run+\"loss_vals_ours.npy\")\n",
    "        performance = np.load(base_str+\"run_\"+run+\"performance_ours.npy\")\n",
    "\n",
    "    #print(performance)\n",
    "    n_highest = 20\n",
    "\n",
    "    to_del = []\n",
    "    for j, p in enumerate(performance):\n",
    "        if len(p[0]) == 2:\n",
    "            print(j)\n",
    "            to_del.append(j)\n",
    "\n",
    "    performance = np.array(np.delete(performance, to_del, axis=0))\n",
    "    param_vals = np.array(np.delete(param_vals, to_del, axis=0))\n",
    "    losses = np.array(np.delete(losses, to_del, axis=0))\n",
    "\n",
    "    proton_test_f1 = np.array([p[0][0] for p in performance[:, 1]])\n",
    "    proton_sort_ind = np.flip(np.argsort(proton_test_f1), 0)\n",
    "    \n",
    "    tot_test_f1 = np.array([p[0].mean() for p in performance[:,1]])\n",
    "    tot_sort_ind = np.flip(np.argsort(tot_test_f1), 0)\n",
    "    sorted_tot_test_f1 = tot_test_f1[tot_sort_ind]\n",
    "    sorted_proton_test_f1 = proton_test_f1[proton_sort_ind]\n",
    "    to_print[i]\n",
    "    #print(param_vals[to_print[i]])\n",
    "    #print(performance[to_print[i]])\n",
    "    \n",
    "    lx  = losses[:, 0]\n",
    "    lx = np.where(lx > 0, lx, 1e5)\n",
    "    best_lx = lx.min(1)\n",
    "    ind_sort_lx = np.argsort(best_lx)\n",
    "    #print(ind_sort_lx[0])\n",
    "    #print(best_lx[ind_sort_lx[0]])\n",
    "    #print(param_vals[ind_sort_lx[0]])\n",
    "    print(\"##########\")\n",
    "    #print(losses.shape)\n",
    "    p = 0\n",
    "    print(sorted_tot_test_f1[0])\n",
    "    print(tot_test_f1[tot_sort_ind[0]], param_vals[tot_sort_ind[0]])\n",
    "    best_lx = losses[tot_sort_ind[p]][0]\n",
    "    print(\"Max reconst loss: \", best_lx.max())\n",
    "    print(\"Reconst loss :\", param_vals[tot_sort_ind[p]][-2])\n",
    "    print(\"test f1 \", tot_test_f1[tot_sort_ind[p]], performance[tot_sort_ind[p]][1][0])\n",
    "    print(\"Ind of experiment: \", tot_sort_ind[p], \"| run: \", run)\n",
    "    print(\"N runs: \", len(proton_sort_ind))\n",
    "    print(\"-----------\")\n",
    "    print(param_vals[tot_sort_ind[0]])\n",
    "\n",
    "    losses = losses[tot_sort_ind][:n_highest]\n",
    "    param_vals = param_vals[tot_sort_ind][:n_highest]\n",
    "    performance = performance[tot_sort_ind][:n_highest]\n",
    "    losses_all.append(losses)\n",
    "    performance_all.append(performance)\n",
    "    params_all.append(param_vals)\n",
    "    \n",
    "    #print(len(sorted_proton_test_f1))\n",
    "    #print(sorted_proton_test_f1)\n",
    "    #print(param_vals[p])\n",
    "    #print(\"performance : \", performance[p])\n",
    "    #print(\"test f1\", performance[p][1][0].mean())\n",
    "    #print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "realevent\n",
      "test f1:  0.8145900844096133\n",
      "[list([[8, 8, 32, 32], [5, 5, 3, 3], [2, 2, 2, 2], [0, 0, 0, 0], 4])\n",
      " list([0.001, 0.0001, 0.7488448785798214, 0.99, 200, 1.188888888888889])\n",
      " {'batchnorm': False, 'include_KL': False, 'include_MMD': True, 'restore_mode': False, 'include_KM:': False, 'simulated_mode': False, 'use_vgg': False, 'use_dd': True}\n",
      " {} 'mse' 'relu']\n",
      "realevent\n",
      "test f1:  0.80448033122487\n",
      "[list([[8, 8, 16, 16], [5, 5, 3, 3], [2, 2, 2, 2], [0, 0, 0, 0], 4])\n",
      " list([0.01, 0.001, 0.7989680735022695, 0.99, 100, 3.366666666666667])\n",
      " {'batchnorm': True, 'include_KL': False, 'include_MMD': False, 'restore_mode': False, 'include_KM:': False, 'simulated_mode': False, 'use_vgg': False, 'use_dd': True}\n",
      " {} 'mse' 'lrelu']\n"
     ]
    }
   ],
   "source": [
    "for i, params in enumerate(params_all):\n",
    "    which = 0\n",
    "    print(datasets[i])\n",
    "    print(\"test f1: \", performance_all[i][which][1][0].mean())\n",
    "    print(params[which])\n",
    "    \n",
    "    \"\"\"\n",
    "    for j, p in enumerate(params):\n",
    "        if 3 == p[0][1][0]:\n",
    "            f1 = performance_all[i][j][1][0].mean()\n",
    "            arch = params_all[i][j]\n",
    "            print(\"#############\")\n",
    "            print(\"test f1 : \", f1)\n",
    "            print(arch)\n",
    "            print(\"#############\")\n",
    "    print(\"--------------------------\")\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8356062231211628"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_all[0][0][1][0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"Simulated\", \"Filtered\", \"Full\"]\n",
    "loss = [\"Reconstruction\", \"Latent\"]\n",
    "\n",
    "fig, axs = plt.subplots(ncols = len(data), nrows=2, figsize=(15, 20))\n",
    "for i in range(len(loss)):\n",
    "    for j in range(len(data)):\n",
    "        ax = axs[i, j]\n",
    "        ax_t = ax.twiny()\n",
    "        performance = np.array([p[1][0].mean() for p in performance_all[j]])\n",
    "        if i == 0:\n",
    "            losses = np.zeros(losses_all[j].shape[0])\n",
    "            for k, lc in enumerate(losses_all[j]):\n",
    "                loss = lc[0]\n",
    "                non_zero = np.nonzero(loss)[0]\n",
    "                to_log = loss[non_zero[-1]]\n",
    "                #print(to_log)\n",
    "                losses[k] = to_log\n",
    "            what_loss = losses > 1     \n",
    "        else:\n",
    "            losses = np.zeros(losses_all[j].shape[0])\n",
    "            for k, lc in enumerate(losses_all[j]):\n",
    "                loss = lc[1]\n",
    "                non_zero = np.nonzero(loss)\n",
    "                if len(non_zero[0]) == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    to_log = loss[non_zero[0][-1]]\n",
    "                    #print(to_log)\n",
    "                    losses[k] = to_log\n",
    "            what_loss = np.zeros(len(losses))\n",
    "            for k, param in enumerate(params_all[j]):\n",
    "                loss = param[2][\"include_MMD\"]\n",
    "                what_loss[k] = 1 if loss else 0 \n",
    "            \n",
    "            what_loss = what_loss.astype(bool)\n",
    "            \n",
    "        outlier_filter = losses < 1e4\n",
    "        one_plot = [losses[np.logical_and(what_loss, outlier_filter)], performance[np.logical_and(what_loss, outlier_filter)]]\n",
    "        two_plot = [losses[np.logical_and(~what_loss, outlier_filter)], performance[np.logical_and(~what_loss, outlier_filter)]]\n",
    "\n",
    "        #print(losses.shape, performance.shape)\n",
    "        ax.scatter(one_plot[0], one_plot[1])\n",
    "        ax_t.scatter(two_plot[0], two_plot[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_crossent(x, y=1): return -(y*np.log(x) + (1-y)*np.log(1-x))\n",
    "def mse(x, y=1): return np.power(x-y, 2)\n",
    "\n",
    "x = np.linspace(-3, 3, 300)\n",
    "y = [0.7, ] #np.linspace(1e-3, 1, 5)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "for y_ in y:\n",
    "    ax[0].plot(x, binary_crossent(x, y_), linewidth=3, color=viridis(0.4))\n",
    "    ax[0].plot([y_,]*2,[0.5, 0.8], label=\"y= \"+str(y_), color=viridis(0.8))\n",
    "    ax[0].set_xlabel(\"x\", size=20)\n",
    "    ax[0].set_ylabel(\"Binary crossentropy\", size=20)\n",
    "    ax[0].legend(fontsize=15)\n",
    "    \n",
    "    ax[1].plot(x, mse(x, y_), linewidth=3, color=viridis(0.4))\n",
    "    ax[1].plot([y_,]*2,[-0.4, 0.8], label=\"y= \"+str(y_), color=viridis(0.8))\n",
    "    ax[1].set_xlabel(\"x\", size=20)\n",
    "    ax[1].set_ylabel(\"Squared error\", size=20)\n",
    "    ax[1].legend(fontsize=15)\n",
    "    \n",
    "    ax[0].tick_params(axis='both', which='major', labelsize=15)\n",
    "    ax[1].tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(performance.shape)\n",
    "print(\"experiments, train/test, scores, classes\")\n",
    "print(performance[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [print(h[-1], h[0]) for h in param_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_names = [r\"$L_x$\", r\"$L_z/\\beta$\"]\n",
    "n_plot = 10\n",
    "fig, ax = plt.subplots(ncols=losses.shape[1], figsize=(16, 9))\n",
    "colors = viridis(np.linspace(0.3, max(sorted_proton_test_f1), n_plot))\n",
    "ax2 = ax[0].twinx()\n",
    "ax3 = ax[1].twinx()\n",
    "\n",
    "#fig.suptitle(\"Loss curves from simulated parameter search\", size=35)\n",
    "\n",
    "label_size = 30\n",
    "ax[1].set_ylabel(r\"Maximum mean discrepancy: $\\bigtriangledown$\", size=label_size)\n",
    "ax3.set_ylabel(r\"Kullback-Leibler : $\\bullet$\", size=label_size)\n",
    "\n",
    "ax2.set_ylabel(r\"Mean squared error: $\\bigtriangleup$\", size=label_size)\n",
    "ax[0].set_ylabel(r\"Binary crossentropy: $\\diamond$\", size=label_size)\n",
    "\n",
    "ax[0].set_title(r\"Reconstruction loss: \"+loss_names[0], size=30)\n",
    "ax[1].set_title(\"Latent loss: \"+loss_names[1], size=30)\n",
    "\n",
    "for j in range(n_plot):\n",
    "    if j == 5:\n",
    "        continue\n",
    "    if j == 8:\n",
    "        continue\n",
    "    for i, a in enumerate(ax):\n",
    "        if i == 1:\n",
    "            beta = param_vals[j][1][0]\n",
    "            mode = param_vals[j][2]\n",
    "            which = None\n",
    "            for l, v in mode.items():\n",
    "                if v:\n",
    "                    if l != \"batchnorm\":\n",
    "                        which = l\n",
    "            if which == \"include_KL\":\n",
    "                fmt = \"o-\"\n",
    "                a = ax3\n",
    "            elif which == \"include_MMD\":\n",
    "                fmt = \"v-\"\n",
    "                a = ax[1]\n",
    "            else:\n",
    "                fmt = \"-\"\n",
    "        else:\n",
    "            beta = 1\n",
    "            if np.any(losses[j, i, :][50:]>1e1):\n",
    "                fmt = \"^-\"\n",
    "            else:\n",
    "                fmt = \"D-\"\n",
    "                a = ax2\n",
    "        loss = losses[j, i, 2:]/beta\n",
    "        a.semilogy(\n",
    "            np.arange(loss.shape[0]),\n",
    "            loss,\n",
    "            fmt,\n",
    "            markevery=250,\n",
    "            color=colors[j],\n",
    "            markersize=15,\n",
    "            linewidth=0.5,\n",
    "            alpha=0.6\n",
    "        )\n",
    "        print(sorted_proton_test_f1[j])\n",
    "        a.tick_params(axis='both', which='major', labelsize=15)\n",
    "        a.set_xlabel(\"epoch\", size=20)\n",
    "\n",
    "#ax[0].set_yscale(\"linear\")\n",
    "ax[0].set_yticks([1e4, 1e3, 1e2])\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../plots/simulated_clf/randomsearch_loss\"+algo+\".png\")\n",
    "plt.savefig(\"../plots/simulated_clf/randomsearch_loss\"+algo+\".pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = [\n",
    "    \"F1 score\",\n",
    "    r\"$L_x$\",\n",
    "    r\"$L_z$\",\n",
    "    \"N parameters\",\n",
    "    \"largest kernel\",\n",
    "    \"N layers\",\n",
    "    \"latent dimension\",\n",
    "    \"latent loss\",\n",
    "    \"reconstruction loss\",\n",
    "    \"activation function\",\n",
    "    \"batchnorm\",\n",
    "    r\"$\\beta$\",\n",
    "    r\"$\\beta_1$\",\n",
    "    r\"$\\eta$\",\n",
    "\n",
    "]\n",
    "\n",
    "categorical_cols = [\n",
    "    \"latent loss\",\n",
    "    \"reconstruction loss\",\n",
    "    \"activation function\",\n",
    "]\n",
    "\n",
    "dataframes = []\n",
    "for j, d in enumerate(datasets):\n",
    "    param_performance = np.zeros((performance.shape[0], len(columns))).astype(object)\n",
    "    performance = performance_all[j]\n",
    "    param_vals = params_all[j]\n",
    "    losses = losses_all[j]\n",
    "    for i in range(performance.shape[0]):\n",
    "        config = param_vals[i]\n",
    "        #print(performance.shape)\n",
    "        p_f1 = performance[i][1].mean() #sorted_proton_test_f1[i]\n",
    "        n_params = 0\n",
    "        for f, k in zip(config[0][0], config[0][1]):\n",
    "            n_params += k**2*f\n",
    "        n_layers = config[0][4]\n",
    "        end_size = 80/(2**n_layers)\n",
    "        beta1 = config[1][2]\n",
    "        beta = config[1][1]\n",
    "        latent_dim = config[1][4]\n",
    "        eta=config[1][1]\n",
    "        mode_config = config[2]\n",
    "        batchnorm = mode_config[\"batchnorm\"]\n",
    "        \n",
    "        lx  = losses[i, 0]\n",
    "        lx = np.where(lx > 0, lx, 1e5)\n",
    "        best_lx = lx.min()\n",
    "        \n",
    "        lz  = losses[i, 1]\n",
    "        lz = np.where(lz > 0, lz, 1e5)\n",
    "        best_lz = lz.min()\n",
    "\n",
    "        latent_loss = \"mmd\" if mode_config[\"include_MMD\"] else \"kld\"\n",
    "        if latent_loss == \"kld\":\n",
    "            latent_loss = latent_loss if mode_config[\"include_KL\"] else \"none\"\n",
    "\n",
    "        reconst_loss = config[4]\n",
    "        if reconst_loss is None:\n",
    "            reconst_loss = \"bce\"\n",
    "\n",
    "        activation = config[5] \n",
    "        param_performance[i] = [\n",
    "            p_f1,\n",
    "            best_lx,\n",
    "            best_lz,\n",
    "            n_params,\n",
    "            config[0][1][0],\n",
    "            n_layers,\n",
    "            latent_dim,\n",
    "            latent_loss,\n",
    "            reconst_loss,\n",
    "            activation,\n",
    "            batchnorm,\n",
    "            beta,\n",
    "            beta1,\n",
    "            eta,\n",
    "        ]\n",
    "\n",
    "    perf_df = pd.DataFrame(param_performance, columns=columns)\n",
    "    dataframes.append(perf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = perf_df.copy()\n",
    "for c in categorical_cols:\n",
    "    dummy = pd.get_dummies(test_df[c])\n",
    "    test_df = pd.concat([test_df, dummy], axis=1)\n",
    "    test_df = test_df.drop(c, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "test_df = test_df.dropna()\n",
    "sns.pairplot(test_df, diag_kind=\"hist\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "test_df = test_df.dropna()\n",
    "test_df[\"const\"] = 1\n",
    "data_cols = [c  for c in test_df.columns if c != \"f1_score\"]\n",
    "#data = test_df[data_cols]\n",
    "#print(test_df.f1_score.values.shape)\n",
    "ols_model = sm.OLS(endog=test_df.f1_score.values.astype(float), exog=test_df[data_cols].values.astype(float))\n",
    "result = ols_model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "test_df = test_df.astype(float)\n",
    "corr_array = np.zeros((len(test_df.columns), 2))\n",
    "for i, c in enumerate(test_df.columns):\n",
    "    w, p = spearmanr(test_df[c], test_df[\"proton f1 score\"])\n",
    "    corr_array[i] = [w, p]\n",
    "\n",
    "corr_m = pd.DataFrame(corr_array, columns=[r\"$\\rho_s$\", \"p\"], index=test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corr_m.to_latex(float_format=lambda x: \"{:.2g}\".format(x),  escape=False),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_vals[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 score</th>\n",
       "      <th>$L_x$</th>\n",
       "      <th>$L_z$</th>\n",
       "      <th>N parameters</th>\n",
       "      <th>largest kernel</th>\n",
       "      <th>N layers</th>\n",
       "      <th>latent dimension</th>\n",
       "      <th>latent loss</th>\n",
       "      <th>reconstruction loss</th>\n",
       "      <th>activation function</th>\n",
       "      <th>batchnorm</th>\n",
       "      <th>$\\beta$</th>\n",
       "      <th>$\\beta_1$</th>\n",
       "      <th>$\\eta$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.815723</td>\n",
       "      <td>0.00423965</td>\n",
       "      <td>0.00336515</td>\n",
       "      <td>976</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>mmd</td>\n",
       "      <td>mse</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.748845</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.80052</td>\n",
       "      <td>0.00412994</td>\n",
       "      <td>100000</td>\n",
       "      <td>976</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>none</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.931548</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.793912</td>\n",
       "      <td>0.00432783</td>\n",
       "      <td>100000</td>\n",
       "      <td>688</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>none</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>True</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.77156</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.797672</td>\n",
       "      <td>0.00417409</td>\n",
       "      <td>0.00729864</td>\n",
       "      <td>2176</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>mmd</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>True</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.441553</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.788598</td>\n",
       "      <td>0.0045926</td>\n",
       "      <td>100000</td>\n",
       "      <td>688</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>none</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.255875</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.780091</td>\n",
       "      <td>0.00512153</td>\n",
       "      <td>100000</td>\n",
       "      <td>1088</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>none</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>False</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.748502</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.78553</td>\n",
       "      <td>0.00570947</td>\n",
       "      <td>0.0241054</td>\n",
       "      <td>1376</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>mmd</td>\n",
       "      <td>mse</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.647847</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.774756</td>\n",
       "      <td>0.0057179</td>\n",
       "      <td>0.0207915</td>\n",
       "      <td>1088</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>mmd</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.374445</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.770226</td>\n",
       "      <td>0.00513076</td>\n",
       "      <td>100000</td>\n",
       "      <td>1088</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>none</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>True</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.768986</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.763657</td>\n",
       "      <td>0.00531745</td>\n",
       "      <td>0.0141592</td>\n",
       "      <td>2176</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>mmd</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>True</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.525585</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.763907</td>\n",
       "      <td>0.00541963</td>\n",
       "      <td>100000</td>\n",
       "      <td>688</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>none</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.331976</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.755668</td>\n",
       "      <td>0.00410553</td>\n",
       "      <td>0.00107543</td>\n",
       "      <td>2176</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>mmd</td>\n",
       "      <td>mse</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.271491</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.757859</td>\n",
       "      <td>0.00453684</td>\n",
       "      <td>0.00236235</td>\n",
       "      <td>1376</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>mmd</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>True</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.598483</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.751815</td>\n",
       "      <td>0.0052313</td>\n",
       "      <td>0.0261567</td>\n",
       "      <td>688</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>mmd</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.38958</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.752279</td>\n",
       "      <td>0.0052577</td>\n",
       "      <td>100000</td>\n",
       "      <td>1376</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>none</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.201815</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.745552</td>\n",
       "      <td>0.00532981</td>\n",
       "      <td>0.0454471</td>\n",
       "      <td>2176</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>mmd</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.425306</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.748109</td>\n",
       "      <td>0.00524046</td>\n",
       "      <td>100000</td>\n",
       "      <td>544</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>none</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>True</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.476656</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.737225</td>\n",
       "      <td>0.541495</td>\n",
       "      <td>100000</td>\n",
       "      <td>2176</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>none</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.948911</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.745834</td>\n",
       "      <td>0.00480458</td>\n",
       "      <td>100000</td>\n",
       "      <td>976</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>none</td>\n",
       "      <td>mse</td>\n",
       "      <td>lrelu</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.645998</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.732279</td>\n",
       "      <td>0.00598124</td>\n",
       "      <td>100000</td>\n",
       "      <td>2176</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>none</td>\n",
       "      <td>mse</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.855601</td>\n",
       "      <td>1e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    F1 score       $L_x$       $L_z$ N parameters largest kernel N layers  \\\n",
       "0   0.815723  0.00423965  0.00336515          976              5        4   \n",
       "1    0.80052  0.00412994      100000          976              5        4   \n",
       "2   0.793912  0.00432783      100000          688              5        4   \n",
       "3   0.797672  0.00417409  0.00729864         2176              5        4   \n",
       "4   0.788598   0.0045926      100000          688              5        4   \n",
       "5   0.780091  0.00512153      100000         1088              5        4   \n",
       "6    0.78553  0.00570947   0.0241054         1376              5        4   \n",
       "7   0.774756   0.0057179   0.0207915         1088              5        4   \n",
       "8   0.770226  0.00513076      100000         1088              5        4   \n",
       "9   0.763657  0.00531745   0.0141592         2176              5        4   \n",
       "10  0.763907  0.00541963      100000          688              5        4   \n",
       "11  0.755668  0.00410553  0.00107543         2176              5        4   \n",
       "12  0.757859  0.00453684  0.00236235         1376              5        4   \n",
       "13  0.751815   0.0052313   0.0261567          688              5        4   \n",
       "14  0.752279   0.0052577      100000         1376              5        4   \n",
       "15  0.745552  0.00532981   0.0454471         2176              5        4   \n",
       "16  0.748109  0.00524046      100000          544              5        4   \n",
       "17  0.737225    0.541495      100000         2176              5        4   \n",
       "18  0.745834  0.00480458      100000          976              5        4   \n",
       "19  0.732279  0.00598124      100000         2176              5        4   \n",
       "\n",
       "   latent dimension latent loss reconstruction loss activation function  \\\n",
       "0               200         mmd                 mse                relu   \n",
       "1               100        none                 mse               lrelu   \n",
       "2               100        none                 mse               lrelu   \n",
       "3                50         mmd                 mse               lrelu   \n",
       "4               200        none                 mse               lrelu   \n",
       "5               150        none                 mse               lrelu   \n",
       "6                50         mmd                 mse                relu   \n",
       "7                10         mmd                 mse               lrelu   \n",
       "8               150        none                 mse               lrelu   \n",
       "9                20         mmd                 mse               lrelu   \n",
       "10               10        none                 mse               lrelu   \n",
       "11               50         mmd                 mse                relu   \n",
       "12               20         mmd                 mse               lrelu   \n",
       "13              200         mmd                 mse               lrelu   \n",
       "14               20        none                 mse               lrelu   \n",
       "15              100         mmd                 mse               lrelu   \n",
       "16               20        none                 mse               lrelu   \n",
       "17              150        none                 mse               lrelu   \n",
       "18               50        none                 mse               lrelu   \n",
       "19               10        none                 mse                relu   \n",
       "\n",
       "   batchnorm $\\beta$ $\\beta_1$  $\\eta$  \n",
       "0      False  0.0001  0.748845  0.0001  \n",
       "1       True  0.0001  0.931548  0.0001  \n",
       "2       True   0.001   0.77156   0.001  \n",
       "3       True   0.001  0.441553   0.001  \n",
       "4      False  0.0001  0.255875  0.0001  \n",
       "5      False   1e-05  0.748502   1e-05  \n",
       "6      False  0.0001  0.647847  0.0001  \n",
       "7      False   0.001  0.374445   0.001  \n",
       "8       True   1e-05  0.768986   1e-05  \n",
       "9       True   0.001  0.525585   0.001  \n",
       "10     False   0.001  0.331976   0.001  \n",
       "11      True   0.001  0.271491   0.001  \n",
       "12      True   0.001  0.598483   0.001  \n",
       "13     False  0.0001   0.38958  0.0001  \n",
       "14      True  0.0001  0.201815  0.0001  \n",
       "15      True  0.0001  0.425306  0.0001  \n",
       "16      True   0.001  0.476656   0.001  \n",
       "17      True     0.1  0.948911     0.1  \n",
       "18      True  0.0001  0.645998  0.0001  \n",
       "19      True   1e-05  0.855601   1e-05  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllllllllllll}\n",
      "\\toprule\n",
      "      F1 score &             $L_x$ &             $L_z$ & N parameters & largest kernel & N layers & latent dimension & latent loss & reconstruction loss & activation function & batchnorm &          $\\beta$ &      $\\beta_1$ &           $\\eta$ \\\\\n",
      "\\midrule\n",
      "$\\num{ 0.99 }$ & $\\num{ 0.00032 }$ &   $\\num{ 1e+05 }$ &         4754 &             17 &        3 &              150 &        none &                 mse &                relu &     False &  $\\num{ 1e-05 }$ & $\\num{ 0.73 }$ &  $\\num{ 1e-05 }$ \\\\\n",
      "$\\num{ 0.99 }$ & $\\num{ 1.1e+03 }$ &      $\\num{ 34 }$ &         3798 &             13 &        6 &               50 &         mmd &                 bce &               lrelu &     False &  $\\num{ 0.001 }$ & $\\num{ 0.82 }$ &  $\\num{ 0.001 }$ \\\\\n",
      "$\\num{ 0.98 }$ & $\\num{ 0.00063 }$ & $\\num{ 0.00028 }$ &         3636 &             15 &        5 &              150 &         mmd &                 mse &               lrelu &     False &  $\\num{ 1e-05 }$ & $\\num{ 0.69 }$ &  $\\num{ 1e-05 }$ \\\\\n",
      "$\\num{ 0.98 }$ & $\\num{ 1.1e+03 }$ &    $\\num{ 0.84 }$ &         2408 &             11 &        3 &                3 &         mmd &                 bce &                relu &      True &    $\\num{ 0.1 }$ & $\\num{ 0.56 }$ &    $\\num{ 0.1 }$ \\\\\n",
      "$\\num{ 0.97 }$ & $\\num{ 1.7e+02 }$ &   $\\num{ 1e+05 }$ &          760 &              7 &        5 &              150 &        none &                 bce &               lrelu &      True &  $\\num{ 1e-05 }$ & $\\num{ 0.25 }$ &  $\\num{ 1e-05 }$ \\\\\n",
      "$\\num{ 0.97 }$ &  $\\num{ 0.0011 }$ &  $\\num{ 0.0038 }$ &         1712 &              7 &        3 &              200 &         mmd &                 mse &                relu &     False &    $\\num{ 0.1 }$ & $\\num{ 0.43 }$ &    $\\num{ 0.1 }$ \\\\\n",
      "$\\num{ 0.95 }$ & $\\num{ 1.6e+02 }$ &      $\\num{ 34 }$ &         1740 &             11 &        5 &               50 &         mmd &                 bce &                relu &     False & $\\num{ 0.0001 }$ & $\\num{ 0.53 }$ & $\\num{ 0.0001 }$ \\\\\n",
      "$\\num{ 0.94 }$ & $\\num{ 0.00076 }$ & $\\num{ 0.00075 }$ &          464 &              7 &        5 &              100 &         mmd &                 mse &                relu &     False & $\\num{ 0.0001 }$ & $\\num{ 0.69 }$ & $\\num{ 0.0001 }$ \\\\\n",
      "$\\num{ 0.94 }$ &  $\\num{ 0.0011 }$ &   $\\num{ 1e+05 }$ &         5194 &             15 &        4 &               50 &        none &                 mse &                relu &      True &    $\\num{ 0.1 }$ & $\\num{ 0.93 }$ &    $\\num{ 0.1 }$ \\\\\n",
      "$\\num{ 0.92 }$ & $\\num{ 1.9e+02 }$ &   $\\num{ 1e+05 }$ &         9266 &             17 &        5 &               10 &        none &                 bce &                relu &      True &  $\\num{ 1e-05 }$ & $\\num{ 0.32 }$ &  $\\num{ 1e-05 }$ \\\\\n",
      "$\\num{ 0.92 }$ & $\\num{ 2.5e+02 }$ &   $\\num{ 0.034 }$ &          272 &              5 &        5 &               50 &         mmd &                 bce &                relu &     False &  $\\num{ 0.001 }$ & $\\num{ 0.46 }$ &  $\\num{ 0.001 }$ \\\\\n",
      "$\\num{ 0.91 }$ & $\\num{ 3.2e+02 }$ & $\\num{ 1.5e+02 }$ &         4770 &             11 &        5 &              200 &         kld &                 bce &                relu &      True &   $\\num{ 0.01 }$ & $\\num{ 0.67 }$ &   $\\num{ 0.01 }$ \\\\\n",
      "$\\num{ 0.91 }$ &  $\\num{ 0.0003 }$ &    $\\num{ 0.01 }$ &          688 &              5 &        4 &              150 &         mmd &                 mse &                relu &      True &  $\\num{ 0.001 }$ & $\\num{ 0.62 }$ &  $\\num{ 0.001 }$ \\\\\n",
      " $\\num{ 0.9 }$ & $\\num{ 1.5e+02 }$ &   $\\num{ 1e+05 }$ &         1524 &             11 &        4 &               10 &        none &                 bce &               lrelu &      True &  $\\num{ 0.001 }$ & $\\num{ 0.51 }$ &  $\\num{ 0.001 }$ \\\\\n",
      " $\\num{ 0.9 }$ & $\\num{ 0.00033 }$ &  $\\num{ 0.0013 }$ &         7346 &             17 &        4 &              150 &         mmd &                 mse &                relu &      True & $\\num{ 0.0001 }$ & $\\num{ 0.71 }$ & $\\num{ 0.0001 }$ \\\\\n",
      "$\\num{ 0.89 }$ & $\\num{ 1.1e+03 }$ &   $\\num{ 0.034 }$ &          688 &              5 &        3 &               50 &         mmd &                 bce &               lrelu &     False &    $\\num{ 0.1 }$ & $\\num{ 0.81 }$ &    $\\num{ 0.1 }$ \\\\\n",
      "$\\num{ 0.88 }$ & $\\num{ 3.8e+02 }$ & $\\num{ 9.1e-06 }$ &         3676 &             15 &        4 &               20 &         kld &                 bce &               lrelu &     False & $\\num{ 0.0001 }$ & $\\num{ 0.28 }$ & $\\num{ 0.0001 }$ \\\\\n",
      "$\\num{ 0.83 }$ & $\\num{ 0.00079 }$ &  $\\num{ 0.0092 }$ &          562 &              7 &        6 &              200 &         mmd &                 mse &               lrelu &     False &  $\\num{ 0.001 }$ & $\\num{ 0.37 }$ &  $\\num{ 0.001 }$ \\\\\n",
      "$\\num{ 0.81 }$ & $\\num{ 7.3e+02 }$ &   $\\num{ 1e+05 }$ &         7316 &             11 &        6 &               20 &         kld &                 bce &                relu &     False &    $\\num{ 0.1 }$ & $\\num{ 0.63 }$ &    $\\num{ 0.1 }$ \\\\\n",
      " $\\num{ 0.8 }$ & $\\num{ 1.1e+03 }$ &   $\\num{ 1e+05 }$ &         3546 &             17 &        5 &               50 &        none &                 bce &               lrelu &     False & $\\num{ 0.0001 }$ & $\\num{ 0.85 }$ & $\\num{ 0.0001 }$ \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataframes[0].to_latex(\n",
    "    #\"../tables/randomsearch_convae_simulated_clf/hyperparams.tex\",\n",
    "    index=False,\n",
    "    longtable=False,\n",
    "    float_format=lambda x: r\"$\\num{{ {:.2g} }}$\".format(x),\n",
    "    escape=False,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../metrics_clf/simulated/f1_scores2000.tex\") as fo:\n",
    "    s = \"\".join(fo.readlines())\n",
    "print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
